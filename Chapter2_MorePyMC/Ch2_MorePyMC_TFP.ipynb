{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "phBEJ8iLIAwF"
   },
   "source": [
    "# Probabilistic Programming and Bayesian Methods for Hackers Chapter  2\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter2_MorePyMC/Ch2_MorePyMC_TFP.ipynb\"><img height=\"32px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter2_MorePyMC/Ch2_MorePyMC_TFP.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Original content ([this Jupyter notebook](https://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter2_MorePyMC/Ch2_MorePyMC_PyMC2.ipynb)) created by Cam Davidson-Pilon ([`@Cmrn_DP`](https://twitter.com/Cmrn_DP))\n",
    "\n",
    "Ported to [Tensorflow Probability](https://www.tensorflow.org/probability/) by Matthew McAteer ([`@MatthewMcAteer0`](https://twitter.com/MatthewMcAteer0)), with help from Bryan Seybold, Mike Shwe ([`@mikeshwe`](https://twitter.com/mikeshwe)), Josh Dillon, and the rest of the TFP team at  Google ([`tfprobability@tensorflow.org`](mailto:tfprobability@tensorflow.org)).\n",
    "\n",
    "Welcome to Bayesian Methods for Hackers. The full Github repository is available at [github/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers). The other chapters can be found on the project's [homepage](https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/). We hope you enjoy the book, and we encourage any contributions!\n",
    "___\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- Dependencies & Prerequisites\n",
    "- A little more on TFP\n",
    "  - TFP Variables\n",
    "    - Initializing Stochastic Variables\n",
    "    - Deterministic variables\n",
    "  - Combining with Tensorflow Core\n",
    "  - Including observations in the Model\n",
    "- Modeling approaches\n",
    "  - Same story; different ending\n",
    "  - Example: Bayesian A/B testing\n",
    "  - A Simple Case\n",
    "    - Execute the TF graph to sample from the posterior\n",
    "  - A and B together\n",
    "    - Execute the TF graph to sample from the posterior\n",
    "- An algorithm for human deceit\n",
    "  - The Binomial Distribution\n",
    "  - Example: Cheating among students\n",
    "    - Execute the TF graph to sample from the posterior\n",
    "  - Alternative TFP Model\n",
    "    - Execute the TF graph to sample from the posterior\n",
    "  - More TFP Tricks\n",
    "  - Example: Challenger Space Shuttle Disaster\n",
    "    - Normal Distributions\n",
    "      - Execute the TF graph to sample from the posterior\n",
    "    - What about the day of the Challenger disaster?\n",
    "    - Is our model appropriate?\n",
    "      - Execute the TF graph to sample from the posterior\n",
    "  - Exercises\n",
    "  - References\n",
    "___\n",
    "\n",
    "This chapter introduces more TFP syntax and variables and ways to think about how to model a system from a Bayesian perspective. It also contains tips and data visualization techniques for assessing goodness-of-fit for your Bayesian model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AIIO6GhdH89m"
   },
   "source": [
    "### Dependencies & Prerequisites\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    Tensorflow Probability is part of the colab default runtime, <b>so you don't need to install Tensorflow or Tensorflow Probability if you're running this in the colab</b>. \n",
    "    <br>\n",
    "    If you're running this notebook in Jupyter on your own machine (and you have already installed Tensorflow), you can use the following\n",
    "    <br>\n",
    "      <ul>\n",
    "    <li> For the most recent nightly installation: <code>pip3 install -q tfp-nightly</code></li>\n",
    "    <li> For the most recent stable TFP release: <code>pip3 install -q --upgrade tensorflow-probability</code></li>\n",
    "    <li> For the most recent stable GPU-connected version of TFP: <code>pip3 install -q --upgrade tensorflow-probability-gpu</code></li>\n",
    "    <li> For the most recent nightly GPU-connected version of TFP: <code>pip3 install -q tfp-nightly-gpu</code></li>\n",
    "    </ul>\n",
    "Again, if you are running this in a Colab, Tensorflow and TFP are already installed\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T15:42:59.228626Z",
     "start_time": "2019-01-12T15:42:57.546286Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T15:43:08.360910Z",
     "start_time": "2019-01-12T15:42:59.228626Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jFKYjxy1IAwG"
   },
   "outputs": [],
   "source": [
    "#@title Imports and Global Variables (run this cell first)  { display-mode: \"form\" }\n",
    "\"\"\"\n",
    "The book uses a custom matplotlibrc file, which provides the unique styles for\n",
    "matplotlib plots. If executing this book, and you wish to use the book's\n",
    "styling, provided are two options:\n",
    "    1. Overwrite your own matplotlibrc file with the rc-file provided in the\n",
    "       book's styles/ dir. See http://matplotlib.org/users/customizing.html\n",
    "    2. Also in the styles is  bmh_matplotlibrc.json file. This can be used to\n",
    "       update the styles in only this notebook. Try running the following code:\n",
    "\n",
    "        import json\n",
    "        s = json.load(open(\"../styles/bmh_matplotlibrc.json\"))\n",
    "        matplotlib.rcParams.update(s)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "#@markdown This sets the warning status (default is `ignore`, since this notebook runs correctly)\n",
    "warning_status = \"ignore\" #@param [\"ignore\", \"always\", \"module\", \"once\", \"default\", \"error\"]\n",
    "import warnings\n",
    "warnings.filterwarnings(warning_status)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(warning_status, category=DeprecationWarning)\n",
    "    warnings.filterwarnings(warning_status, category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "#@markdown This sets the styles of the plotting (default is styled like plots from [FiveThirtyeight.com](https://fivethirtyeight.com/))\n",
    "matplotlib_style = 'fivethirtyeight' #@param ['fivethirtyeight', 'bmh', 'ggplot', 'seaborn', 'default', 'Solarize_Light2', 'classic', 'dark_background', 'seaborn-colorblind', 'seaborn-notebook']\n",
    "import matplotlib.pyplot as plt; plt.style.use(matplotlib_style)\n",
    "import matplotlib.axes as axes;\n",
    "from matplotlib.patches import Ellipse\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set_context('notebook')\n",
    "from IPython.core.pylabtools import figsize\n",
    "#@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)\n",
    "notebook_screen_res = 'retina' #@param ['retina', 'png', 'jpeg', 'svg', 'pdf']\n",
    "%config InlineBackend.figure_format = notebook_screen_res\n",
    "\n",
    "import tensorflow as tf\n",
    "tfe = tf.contrib.eager\n",
    "\n",
    "# Eager Execution\n",
    "#@markdown Check the box below if you want to use [Eager Execution](https://www.tensorflow.org/guide/eager)\n",
    "#@markdown Eager execution provides An intuitive interface, Easier debugging, and a control flow comparable to Numpy. You can read more about it on the [Google AI Blog](https://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html)\n",
    "use_tf_eager = False #@param {type:\"boolean\"}\n",
    "\n",
    "# Use try/except so we can easily re-execute the whole notebook.\n",
    "if use_tf_eager:\n",
    "    try:\n",
    "        tf.enable_eager_execution()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "  \n",
    "def evaluate(tensors):\n",
    "    \"\"\"Evaluates Tensor or EagerTensor to Numpy `ndarray`s.\n",
    "    Args:\n",
    "    tensors: Object of `Tensor` or EagerTensor`s; can be `list`, `tuple`,\n",
    "        `namedtuple` or combinations thereof.\n",
    "\n",
    "    Returns:\n",
    "        ndarrays: Object with same structure as `tensors` except with `Tensor` or\n",
    "          `EagerTensor`s replaced by Numpy `ndarray`s.\n",
    "    \"\"\"\n",
    "    if tf.executing_eagerly():\n",
    "        return tf.contrib.framework.nest.pack_sequence_as(\n",
    "            tensors,\n",
    "            [t.numpy() if tf.contrib.framework.is_tensor(t) else t\n",
    "             for t in tf.contrib.framework.nest.flatten(tensors)])\n",
    "    return sess.run(tensors)\n",
    "\n",
    "class _TFColor(object):\n",
    "    \"\"\"Enum of colors used in TF docs.\"\"\"\n",
    "    red = '#F15854'\n",
    "    blue = '#5DA5DA'\n",
    "    orange = '#FAA43A'\n",
    "    green = '#60BD68'\n",
    "    pink = '#F17CB0'\n",
    "    brown = '#B2912F'\n",
    "    purple = '#B276B2'\n",
    "    yellow = '#DECF3F'\n",
    "    gray = '#4D4D4D'\n",
    "    def __getitem__(self, i):\n",
    "        return [\n",
    "            self.red,\n",
    "            self.orange,\n",
    "            self.green,\n",
    "            self.blue,\n",
    "            self.pink,\n",
    "            self.brown,\n",
    "            self.purple,\n",
    "            self.yellow,\n",
    "            self.gray,\n",
    "        ][i % 9]\n",
    "TFColor = _TFColor()\n",
    "\n",
    "def session_options(enable_gpu_ram_resizing=True, enable_xla=True):\n",
    "    \"\"\"\n",
    "    Allowing the notebook to make use of GPUs if they're available.\n",
    "    \n",
    "    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear \n",
    "    algebra that optimizes TensorFlow computations.\n",
    "    \"\"\"\n",
    "    config = tf.ConfigProto()\n",
    "    config.log_device_placement = True\n",
    "    if enable_gpu_ram_resizing:\n",
    "        # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
    "        # GPU. Otherwise the colab malloc's all GPU ram.\n",
    "        config.gpu_options.allow_growth = True\n",
    "    if enable_xla:\n",
    "        # Enable on XLA. https://www.tensorflow.org/performance/xla/.\n",
    "        config.graph_options.optimizer_options.global_jit_level = (\n",
    "            tf.OptimizerOptions.ON_1)\n",
    "    return config\n",
    "\n",
    "\n",
    "def reset_sess(config=None):\n",
    "    \"\"\"\n",
    "    Convenience function to create the TF graph & session or reset them.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = session_options()\n",
    "    global sess\n",
    "    tf.reset_default_graph()\n",
    "    try:\n",
    "        sess.close()\n",
    "    except:\n",
    "        pass\n",
    "    sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "reset_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24368dz9IAwM"
   },
   "source": [
    "## A little more on TensorFlow and TensorFlow Probability\n",
    "\n",
    "To explain TensorFlow Probability, it's worth going into the various methods of working with Tensorflow tensors. Here, we introduce the notion of Tensorflow graphs and how we can use certain coding patterns to make our tensor-processing workflows much faster and more elegant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tOj8bjBmNuxm"
   },
   "source": [
    "### TensorFlow Graph and Eager Modes\n",
    "\n",
    "TFP accomplishes most of its heavy lifting via the main `tensorflow` library. The `tensorflow` library also contains many of the familiar computational elements of NumPy and uses similar notation. While NumPy directly executes computations (e.g. when you run `a + b`), `tensorflow` in graph mode instead builds up a \"compute graph\" that tracks that you want to perform the `+` operation on the elements `a` and `b`. Only when you evaluate a `tensorflow` expression  does the computation take place--`tensorflow` is lazy evaluated. The benefit of using Tensorflow over NumPy is that the graph enables mathematical optimizations (e.g. simplifications), gradient calculations via automatic differentiation, compiling the entire graph to C to run at machine speed, and also compiling it to run on a GPU or TPU. \n",
    "\n",
    "Fundamentally, TensorFlow uses [graphs](https://www.tensorflow.org/guide/graphs) for computation, wherein the graphs represent computation as dependencies among individual operations. In the programming paradigm for Tensorflow graphs, we first define the dataflow graph, and then create a TensorFlow session to run parts of the graph. A Tensorflow [`tf.Session()`](https://www.tensorflow.org/api_docs/python/tf/Session) object runs the graph to get the variables we want to model. In the example below, we are using a global session object `sess`, which we created above in the \"Imports and Global Variables\" section. \n",
    "\n",
    "To avoid the sometimes confusing aspects of lazy evaluation, Tensorflow's eager mode does immediate evaluation of results to give an even more similar feel to working with NumPy. With Tensorflow [eager](https://www.tensorflow.org/guide/eager) mode, you can evaluate operations immediately, without explicitly building graphs: operations return concrete values instead of constructing a computational graph to run later. If we're in eager mode, we are presented with tensors that can be converted to numpy array equivalents immediately. Eager mode makes it easy to get started with TensorFlow and debug models.\n",
    "\n",
    "\n",
    "TFP is essentially:\n",
    "\n",
    "* a collection of tensorflow symbolic expressions for various probability distributions that are combined into one big compute graph, and\n",
    "* a collection of inference algorithms that use that graph to compute probabilities and gradients.\n",
    "\n",
    "For practical purposes, what this means is that in order to build certain models we sometimes have to use core Tensorflow. This simple example for Poisson sampling is how we might work with both graph and eager modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T15:43:08.480694Z",
     "start_time": "2019-01-12T15:43:08.364427Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CmiGas0kXiEw",
    "outputId": "6ecefef8-ec11-4048-e840-15c875565a8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of sample from data generator: 0.0\n"
     ]
    }
   ],
   "source": [
    "parameter = tfd.Exponential(rate=1., name=\"poisson_param\").sample()\n",
    "data_generator = tfd.Poisson(parameter, name=\"data_generator\")\n",
    "data_generator_samples = data_generator.sample()\n",
    "\n",
    "if tf.executing_eagerly():\n",
    "    data_gen_samps_ = tf.contrib.framework.nest.pack_sequence_as(\n",
    "        data_generator_samples,\n",
    "        [t.numpy() if tf.contrib.framework.is_tensor(t) else t\n",
    "         for t in tf.contrib.framework.nest.flatten(data_generator_samples)])\n",
    "else:\n",
    "    data_gen_samps_ = sess.run(data_generator_samples)\n",
    "    \n",
    "print(\"Value of sample from data generator:\", data_gen_samps_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9kArT4GTIAwT"
   },
   "source": [
    "In graph mode, Tensorflow will automatically assign any variables to a graph; they can then be evaluated in a session or made available in eager mode. If you try to define a variable when the session is already closed or in a finalized state, you will get an error. In the \"Imports and Global Variables\" section, we defined a particular type of session, called [`InteractiveSession`](https:///www.tensorflow.org/api_docs/python/tf/InteractiveSession). \n",
    "This defnition of a global `InteractiveSession` allows us to access our session variables interactively via a shell or notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4IEk40NbIAwX"
   },
   "source": [
    "Using the pattern of a global session, we can incrementally build a graph and run subsets of it to get the results.\n",
    "\n",
    "Eager execution further simplifies our code, eliminating the need to call session functions explicitly. In fact, if you try to run graph mode semantics in eager mode, you will get an error message like this:\n",
    "\n",
    "```\n",
    "AttributeError: Tensor.graph is meaningless when eager execution is enabled.\n",
    "```\n",
    "\n",
    "As mentioned in the previous chapter, we have a nifty tool that allows us to create code that's usable in both graph mode and eager mode. The custom `evaluate()` function allows us to evaluate tensors whether we are operating in TF graph or eager mode. The function looks like the following:\n",
    "\n",
    "```python\n",
    "\n",
    "def evaluate(tensors):\n",
    "    if tf.executing_eagerly():\n",
    "         return tf.contrib.framework.nest.pack_sequence_as(\n",
    "             tensors,\n",
    "             [t.numpy() if tf.contrib.framework.is_tensor(t) else t\n",
    "             for t in tf.contrib.framework.nest.flatten(tensors)])\n",
    "    with tf.Session() as sess:\n",
    "        return sess.run(tensors)\n",
    "\n",
    "```\n",
    "\n",
    "Each of the tensors corresponds to a NumPy-like output. To distinguish the tensors from their NumPy-like counterparts, we will use the convention of appending an underscore to the version of the tensor that one can use NumPy-like arrays on. In other words, the output of `evaluate()` gets named as `variable` + `_` = `variable_` . Now, we can do our Poisson sampling using both the `evaluate()` function and this new convention for naming Python variables in TFP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T15:43:08.580698Z",
     "start_time": "2019-01-12T15:43:08.480694Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Bk-vyPB9IAwX",
    "outputId": "2b934b1e-c0a5-4b92-e6d2-a959b281c354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from exponential distribution before evaluation:  Tensor(\"poisson_param_1/sample/Reshape:0\", shape=(), dtype=float32)\n",
      "Evaluated sample from exponential distribution:  2.4235342\n"
     ]
    }
   ],
   "source": [
    "# Defining our Assumptions\n",
    "parameter = tfd.Exponential(rate=1., name=\"poisson_param\").sample()\n",
    "\n",
    "# Converting our TF to Numpy\n",
    "[ parameter_ ] = evaluate([ parameter ])\n",
    "\n",
    "print(\"Sample from exponential distribution before evaluation: \", parameter)\n",
    "print(\"Evaluated sample from exponential distribution: \", parameter_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZlGWIiPLIAwo"
   },
   "source": [
    "More generally, we can use our `evaluate()` function to convert between the Tensorflow `tensor` data type and one that we can run operations on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T15:43:08.741032Z",
     "start_time": "2019-01-12T15:43:08.580698Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1tzQmnsFIAwp",
    "outputId": "c7c87627-826f-445d-e000-843894eecc16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'parameter_' evaluated Tensor : 0.34744835\n",
      "'data_generator_' sample evaluated Tensor : 0.0\n"
     ]
    }
   ],
   "source": [
    "[ \n",
    "    parameter_,\n",
    "    data_generator_sample_,\n",
    "] = evaluate([ \n",
    "    parameter, \n",
    "    data_generator.sample(),\n",
    "])\n",
    "\n",
    "print(\"'parameter_' evaluated Tensor :\", parameter_)\n",
    "print(\"'data_generator_' sample evaluated Tensor :\", data_generator_sample_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0PLxpCIc--r"
   },
   "source": [
    "\n",
    "A general rule of thumb for programming in TensorFlow is that if you need to do any array-like calculations that would require NumPy functions, you should use their equivalents in TensorFlow. This practice is necessary because NumPy can produce only constant values but TensorFlow tensors are a dynamic part of the computation graph. If you mix and match these the wrong way, you will typically get an error about incompatible types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqkS8vztNoyh"
   },
   "source": [
    "### TFP Distributions\n",
    "\n",
    "Let's look into how [`tfp.distributions`](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions) work.\n",
    "\n",
    "TFP uses distribution subclasses to represent *stochastic*, random variables. A variable is stochastic when the following is true: even if you knew all the values of the variable's parameters and components, it would still be random. Included in this category are instances of classes [`Poisson`](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Poisson), [`Uniform`](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Uniform), and [`Exponential`](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Exponential).\n",
    "\n",
    "You can draw random samples from a stochastic variable. When you draw samples, those samples become [`tensorflow.Tensors`](https://www.tensorflow.org/api_docs/python/tf/Tensor) that behave deterministically from that point on. A quick mental check to determine if something is *deterministic* is: *If I knew all of the inputs for creating the variable `foo`, I could calculate the value of `foo`.*  You can add, subtract, and otherwise manipulate the tensors in a variety of ways discussed below. These operations are almost always deterministic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NdKiqWtWIAwy"
   },
   "source": [
    "#### Initializing a Distribution\n",
    "\n",
    "Initializing a stochastic, or random, variable requires a few class-specific parameters that describe the Distribution's shape, such as the location and scale. For example:\n",
    "\n",
    "```python\n",
    "some_distribution = tfd.Uniform(0., 4.)\n",
    "```\n",
    "\n",
    "initializes a stochastic, or random, [`Uniform`](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Uniform) distribution with the lower bound at 0 and upper bound at 4. Calling `sample()` on the distribution returns a tensor that will behave deterministically from that point on:\n",
    "\n",
    "```python\n",
    "sampled_tensor = some_distribution.sample()\n",
    "```\n",
    "\n",
    "The next example demonstrates what we mean when we say that distributions are stochastic but tensors are deterministic:\n",
    "\n",
    "```\n",
    "derived_tensor_1 = 1 + sampled_tensor\n",
    "derived_tensor_2 = 1 + sampled_tensor  # equal to 1\n",
    "\n",
    "derived_tensor_3 = 1 + some_distribution.sample()\n",
    "derived_tensor_4 = 1 + some_distribution.sample()  # different from 3\n",
    "```\n",
    "\n",
    "The first two lines produce the same value because they refer to the same sampled tensor. The last two lines likely produce different values because they refer to independent samples drawn from the same distribution.\n",
    "\n",
    "To define a multiviariate distribution, just pass in arguments with the shape you want the output to be when creating the distribution. For example:\n",
    "\n",
    "```python\n",
    "betas = tfd.Uniform([0., 0.], [1., 1.])\n",
    "```\n",
    "\n",
    "Creates a Distribution with batch_shape (2,). Now, when you call betas.sample(),\n",
    "two values will be returned instead of one. You can read more about TFP shape semantics in the [TFP docs](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Understanding_TensorFlow_Distributions_Shapes.ipynb), but most uses in this book should be self-explanatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UPt9k8YrIAwz"
   },
   "source": [
    "#### Deterministic variables\n",
    "\n",
    "We can create a deterministic distribution similarly to how we create a stochastic distribution. We simply call up the [`Deterministic`](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Deterministic) class from Tensorflow Distributions and pass in the deterministic value that we desire\n",
    "```python\n",
    "deterministic_variable = tfd.Deterministic(name=\"deterministic_variable\", loc=some_function_of_variables)\n",
    "```\n",
    "\n",
    "Calling `tfd.Deterministic` is useful for creating distributions that always have the same value. However, the much more common pattern for working with deterministic variables in TFP is to create a tensor or sample from a distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T15:43:08.801045Z",
     "start_time": "2019-01-12T15:43:08.745045Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "feDM_HX6IAw0"
   },
   "outputs": [],
   "source": [
    "lambda_1 = tfd.Exponential(rate=1., name=\"lambda_1\") #stochastic variable\n",
    "lambda_2 = tfd.Exponential(rate=1., name=\"lambda_2\") #stochastic variable\n",
    "tau = tfd.Uniform(name=\"tau\", low=0., high=10.) #stochastic variable\n",
    "\n",
    "# deterministic variable since we are getting results of lambda's after sampling    \n",
    "new_deterministic_variable = tfd.Deterministic(name=\"deterministic_variable\", \n",
    "                                               loc=(lambda_1.sample() + lambda_2.sample()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cRzLJmAJIAw3"
   },
   "source": [
    "The use of the deterministic variable was seen in the previous chapter's text-message example.  Recall the model for $\\lambda$ looked like: \n",
    "\n",
    "$$\n",
    "\\lambda = \n",
    "\\begin{cases}\\lambda_1  & \\text{if } t \\lt \\tau \\cr\n",
    "\\lambda_2 & \\text{if } t \\ge \\tau\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "And in TFP code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T15:43:09.081197Z",
     "start_time": "2019-01-12T15:43:08.804441Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "IXdTQeqrIAw3",
    "outputId": "6ab97e74-6a36-4b27-fe7f-c1d4e6ea30be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 samples from our deterministic lambda model: \n",
      " [0.50651413 0.39516008 0.39516008 0.39516008 0.39516008]\n"
     ]
    }
   ],
   "source": [
    "n_data_points = 5  # in CH1 we had ~70 data points\n",
    "idx = np.arange(n_data_points)\n",
    "\n",
    "lambda_deterministic = tfd.Deterministic(tf.gather([lambda_1.sample(), lambda_2.sample()],\n",
    "                    indices=tf.to_int32(\n",
    "                        tau.sample() >= idx)))\n",
    "[lambda_deterministic_] = evaluate([lambda_deterministic.sample()])\n",
    "\n",
    "print(\"{} samples from our deterministic lambda model: \\n\".format(n_data_points), lambda_deterministic_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EFgJwLATIAw8"
   },
   "source": [
    "Clearly, if $\\tau, \\lambda_1$ and $\\lambda_2$ are known, then $\\lambda$ is known completely, hence it is a deterministic variable. We use indexing here to switch from $\\lambda_1$ to $\\lambda_2$ at the appropriate time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IMNdtRTtIAxB"
   },
   "source": [
    "### Including observations in the model\n",
    "\n",
    "At this point, it may not look like it, but we have fully specified our priors. For example, we can ask and answer questions like \"What does my prior distribution of $\\lambda_1$ look like?\" \n",
    "\n",
    "To do this, we will sample from the distribution. The method `.sample()` has a very simple role: get data points from the given distribution. We can then evaluate the resulting tensor to get a NumPy array-like object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T15:43:09.420684Z",
     "start_time": "2019-01-12T15:43:09.081197Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "VNdQVTSFIAxC",
    "outputId": "c73c52bd-11cf-4d81-946d-f947eb9c0d59"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABngAAAKECAYAAAAkHjTaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3X+4p2VdJ/D3B0YNHWFUMnJQwCC6ln6Mqa2aMSBa7a6tKbruupVMbbUoqVe/lNYK0yulqF03kH4CYbmtUJlu6YYtgyDUheuPyA0XlUGdgCSdgWFw+HXvH89zPGcPc86cH9+Z77mH1+u6znWf7/e5n/v+PGe+N9dw3nM/T7XWAgAAAAAAQD8OmXYBAAAAAAAALI+ABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM4IeAAAYA2oqq1V1arqjGnXshJj7a2qjp33/pq8rsXqqqpt47FTDnxl+7ZWf6aLqarHVtWvV9Vnquresf5t065rvqr6o7G2L1bVO6vq8GnXBAAAC1k37QIAAKAHVXVJklfu5dBdST6b5Iokb2+tfeFA1vVwMgYaxyZ5T2vt49OtZvIO8uv7kyTPH7+/M8mXknxxeuUs6OuS3J3kyCQ/kGRXkjOnWhEAACzADh4AAFie+5LcPn79Y5L1Sb4tyU8nuaGqnrvCcT+X5FNJdk6iyDVkktd1RpJfTLJpAmOtxZ/3GVna9a3F2hdUVSdlCHfuS/Ls1toRrbWjWmvPnHJpD9FaOzXJhiSXj2/9yymWAwAAi7KDBwAAlufa1topMy+q6tFJTk/yXzP8Yviyqnpqa+2e5QzaWvuhiVa5RqzV61qrdS1Fh7WfNLZ/21r766lWsgSttfur6reSvDTJU6rq8NbandOuCwAA5rODBwAAVqG1tru19s4krxnfOirJ90+xJFhrDhvbXVOtYnlumPP9SQv2AgCAKRLwAADAZLw7yYPj90+febOqto0PbT+lqjZW1Tuq6rNVtaeqPj6n39ax3xkLTVBVL6mqD4wPgN9TVV+oqj+sqm9foP+S5l6Kqjqkqn6iqj5RVfeMNbyvqp69j/MWvK6qemRVvbaqrq2qHVV1X1XdPs5xwczYVXVGVbUkm8dTLx7HnPnattzrXsrPe+z3lKr63ar6fFV9papurqrzquqIBfrP1HTsAsePnekz573lXt/+/qw8vqp+fbzWPVW1vap+p6q+frGf1V7GPGe8rkvGtzbPu65TJlz3qj7jc7XWbs/wnKAk+eaVjgMAAPuTW7QBAMAEtNb2VNUdSZ6Y5PC9dPnGJJdleHj77gzPI1mSqjokycVJZm7N9UCSu5JsTPKKJP+2qs5qrV24wBArnnucf12GZ5K8aHzr/gz/L/HCJN9bVS9fznhzxvzLzIYaLcMzZZ6Q4Wf4reP31yW5J8Mzjx6f5BFJ7hzfm/HFBaZZ1XUnOT5DcPe1GXaftCTHJvmpJC+qqpNba7cuc8y9Wen1PcQEPitHZwhkjsnwM2tJnpTkPyR5flV9e2vty0ssZ1eG6zosw5q4L7OhSZLcO8G6V/tn/f+pqlMz/HkkAh4AANYoO3gAAGACquqwDEFAkuzYS5dfS3Jrku9srT2mtbY+wzM+luJnM/ziuyX5+SSPa609LsMv4y/L8Pf686vq5AXOX83cSfL6DOHOg0l+JskR4/xPTfLBJBctY6wZr8gQ7uxO8oNJHj2O+agM4cJZST6RJK21/95aOyrJteO5r22tHTXn65kLzLHa6z4vQ+j0Xa21xyZ5TIbb792RIfz5/WWMtaBVXN/erPaz8htJvpzkOa21xyRZn+HPfkeGcOvsZVzXeeN1vXZ869p513XtnO7T/ox/VVU9KsncIEnAAwDAmiTgAQCAyfiRJDV+/zd7OX5/khfM/aV2a+3T+xq0qh6T2V+qn9tae0tr7a7x/O1J/l2SazL83f4tCwyzornnzP+z48s3j7+03z2OcXOGwGP7Usaa51lje2lr7Q9aa18Zx3ygtfa51toFrbW3rmDcuVZ83aNHJfkXrbVrxnMfbK39WZJ/Mx5/QVU9d5U1TsyEPit7kjy/tXbdeN79rbX3zum/otDkANS92j/ruX4uyYkZwqZEwAMAwBol4AEAgBWqwbFV9dNJfmV8+5Yk79tL90vH53os13dnuL3VvXPm+KrW2gNJ3jy+/K6qOmqCc8+df0+S/7yX+fdk2OmyXHeO7bKe67JMq7nuJHn33kKC1tqVmd1tM/HAYxUm8Vn57dbaP+3l/feM7XFjIDNJ0/6Mf1VVnZhhx1pL8qbx7SdW1dcufBYAAEyHgAcAAJbnqw+Kz3DLspuT/GqG54zcmuT7W2v37uW861Y438zD5T+xyLNPPpRhB8Pc/pOYe+54H2+t7Vygz1UrGPf9Y/uiqnpvVb2kqp6wgnEWs5rrTpKtixybuea9/bynZRKflesXOG/uLq0NK6htMdP+jM/1mxl2bl2U4XZ1Mx6yi6eq1lfVm6rqL6rqi+N/F94woToAAGCfBDwAALA892V4cPztSW5L8pkkV2S4jdlJrbWPL3DeF1c438zOgQVvgzbe3mxm18XedhqsdO654/3DIn2WfYu21tpVSX4hwy/tvy/JHye5o6r+vqrOq6oTll3pQ63mupPFr2vm2Fra2TGJz8pdi5w34xErqm5h0/6MJ0mqakuSUzKs7Z9prX0ps5/7vd2m7cgMn+FvSfKx1c4PAADLtW7aBQAAQGeuba2dsoLzHljlvI9axbmrnXu/aK29uar+IMnLM/xi/dlJvmn8em1V/Uhr7dJVTLE/r7v23WVqVvNZmaapfcar6sgMO/GS5HVzdhLdkORJ2XvAc2uSja21f6iqYzPs5gMAgAPGDh4AAFjbZnYmHLNQh6r6miQztzdb9U6GBeZ/0iJ9Fju2qNbaza21t7XWvjfJ45OcmuF2XOuSvKOqnrjSsSdgseuaeXbQ/J/3TNDwNQucd8SqKlrctD8rK7UW6v61cfz3t9b+aM77fzu2Dwl4Wmt7WmuL7WwDAID9SsADAABr20fH9oSq2rhAn5Mzuzv/owv0We38m6rq8AX6bJ7ERK21B1prW5O8MMOt8B6T5Blzujw4tgdq98xi1zVzbP7Pe8fYHr3Aec9cZMzVXt+0PysrNdW6q+rUJD+U5O4kZ847fMPYnjTJOQEAYBIEPAAAsLb9ZZI7Mzz35GfmH6yqQ5P8/Pjy6tbabROe/3+O8z8qyWv3Mv8jk/zUcgcdz1vIvZndCTP3tl13ju2G5c63Qi+vqqfOf7OqTk7ynePLy+YdngkEXrSX8x6V5HWLzLfa65v2Z2Wlplb3+Gdy4fjyF1prt8zrMrOD54iqevKk5gUAgEkQ8AAAwBrWWrs7yS+PL19TVf+pqtYnybjb4b8leW6G3R9v3A/z707yK+PLX6yqn6yqw8b5j03yp0lW8ovvS6vq4qr6nqp67Myb45i/n+EWZ/ckuXrOOZ8c25dU1f681dmMe5O8v6qeM9Z2SFV9X5LLx+NXtNY+PO+cd4/tj1bVljFASFWdlOQvsvht31Z1fdP+rKzUlOv+uSQnZtgV9Pa9HP/7JPeP3+/tOTwAADA1Ah4AAFj7zktyaYZbd70lyY6q+lKSzyd5WYZffP9Ea+1D+2n+c5P8WZJDMzyr5M6q+nKGh8p/d5IfXsGYX5PkjCQfSLKzqr5cVXePY748ww6eH2+t3THnnHdmCF2em+SOqtpeVduq6pqVXdY+/XSSxyX5cFXdlWRXkvcm+dokn07yyr2c87tJ/ibDzqOLkuyqqp1J/i7JpiRbFplvEtc37c/KSh3wuqvqxCSvz/BZ+9HW2gPz+7TW7k3yqfGlgAcAgDVFwAMAAGvc+GyaVyZ5aYbbWe1Isj7JrRl2N3xHa+0d+3H++5OcnuQ1GW5ZdX+GX4r/eZLNrbU/WcGwb0jysxkCns8meWSGAOkzSS5O8u2ttXfOq+PGJC8Yz9mZ5Kgkx2Th592s1qczPAPoonG+Q5NsyxByPaO1duv8E1pr9401/urY98EMz3a5JMnTk3xiockmcX3T/qys1JTq/s0MQdx/aa0t9lyfmdvuCXgAAFhTqrU27RoAAACgW+OtBW9OcnZr7W3TrQYAgIcLO3gAAAAAAAA6s27aBQAAAECPquqsJBvGryQ5tapm/j/7N1prO6dTGQAADwdu0QYAAAArUFXbMjwnaW+Oa61tO3DVAADwcCPgAQAAAAAA6Ixn8AAAAAAAAHRGwAMAAAAAANAZAQ8AAAAAAEBnBDwAAAAAAACdEfAAAAAAAAB0RsADAAAAAADQmXXTLmCt2blz58eSHJdkV5JPT7kcAAAAAACgb8cnWZ/k5iOOOOJpkxpUwPNQxyU5YvzaOOVaAAAAAACAg8NxkxzMLdoeate0C4C1YPfu3dm9e/e0y4A1wXqAWdYDzLIeYGAtwCzrAWZZDzDrgQcemPl2ovmDgOeh3JYNkmzfvj3bt2+fdhmwJlgPMMt6gFnWAwysBZhlPcAs6wFm7dmzZ+bbieYPAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOjMumkXsFZdc+uevPADd+zXOXZs2bhfxwcAAAAAAA5OdvAAAAAAAAB0RsADAAAAAADQGQEPAAAAAABAZwQ8AAAAAAAAnRHwAAAAAAAAdEbAAwAAAAAA0BkBDwAAAAAAQGcEPAAAAAAAAJ0R8AAAAAAAAHRGwAMAAAAAANAZAQ8AAAAAAEBnBDwAAAAAAACdEfAAAAAAAAB0RsADAAAAAADQGQEPAAAAAABAZwQ8AAAAAAAAnRHwAAAAAAAAdEbAAwAAAAAA0BkBDwAAAAAAQGcEPAAAAAAAAJ0R8AAAAAAAAHRGwAMAAAAAANAZAQ8AAAAAAEBnBDwAAAAAAACdEfAAAAAAAAB0RsADAAAAAADQGQEPAAAAAABAZwQ8AAAAAAAAnRHwAAAAAAAAdEbAAwAAAAAA0BkBDwAAAAAAQGcEPAAAAAAAAJ0R8AAAAAAAAHRGwAMAAAAAANAZAQ8AAAAAAEBnBDwAAAAAAACdEfAAAAAAAAB0RsADAAAAAADQGQEPAAAAAABAZwQ8AAAAAAAAnRHwAAAAAAAAdEbAAwAAAAAA0BkBDwAAAAAAQGcEPAAAAAAAAJ0R8AAAAAAAAHRGwAMAAAAAANAZAQ8AAAAAAEBnBDwAAAAAAACdEfAAAAAAAAB0RsADAAAAAADQGQEPAAAAAABAZwQ8AAAAAAAAnRHwAAAAAAAAdEbAAwAAAAAA0BkBDwAAAAAAQGcEPAAAAAAAAJ0R8AAAAAAAAHRGwAMAAAAAANAZAQ8AAAAAAEBnBDwAAAAAAACdEfAAAAAAAAB0RsADAAAAAADQGQEPAAAAAABAZwQ8AAAAAAAAnRHwAAAAAAAAdEbAAwAAAAAA0BkBDwAAAAAAQGcmGvBU1Suq6uqq2llVu6rqI1X16qpa9jxV9biq+uWquqGq7q6qPVV1S1W9s6o2TbJuAAAAAACAnkws4KmqC5L8YZJnJLk6yRVJvjHJ+Ukur6pDlzHWU5J8PMnZSY5KcmWS9yW5L8kPJLm+qk6fVO0AAAAAAAA9mUjAM4Ytr0pyW5Jvba29sLX24iQnJPn7JC9OctYyhnxbkqck+Yskx4zjvTRDYPSmJOuS/FZVPWIS9QMAAAAAAPRkUjt4zh7b17fWbpp5s7V2e5Izx5dvWMat2k4d2ze31nbPGe/BJG9Ock+SJ2QIkAAAAAAAAB5W1q12gKo6OsnTk9yb5LL5x1trV1XV9iQbkzwrybVLGHbPEqe/Y6l1rkUbLt5+wObasWXjAZsLAAAAAADYvyaxg+dpY/vJ1to9C/S5fl7fffnA2L6xqh4982ZVVZJfSHJYkve21v5xucUCAAAAAAD0btU7eJIcN7a3LNLnc/P67ssbM4RB/yrJLVX11xl29XxbkmOS/EGGZ/4AAAAAAAA87Ewi4Fk/tncv0mfX2D52KQO21u6oqucluSDJK5O8cM7hTyW5qrV211ILrKozkpyxlL5bt27dtGnTpqUO3Y2bbrpp351gL3x2YJb1ALOsB5hlPcDAWoBZ1gPMsh4g2bhx/zxCZRIBT41tm8BYw4BV35TkvRkCoR9M8sEk92R41s+vJvmdqnpOa+2HlzjksUk2L6Xjrl279t0JAAAAAABgiiYR8MzspFm/SJ+ZY/vcdVNV65L8cZLjk3xna+26OYf/V1W9IMn/SbKlqt7ZWrtyCTVuS3LVEvpl/fr1m5IcsZS+PTnhhBOmXQKdmfnXFT47YD3AXNYDzLIeYGAtwCzrAWZZDzBr9+7d+2XcSQQ828b2mEX6PHle38X88yT/LMln54U7SZLW2peq6v0Zbrn2/CT7DHhaa5ckuWQJc2fnzp1bs8TdPgAAAAAAANNwyATG+NjYnlRVhy3Q55nz+i7mKWO7c5E+O8b28UsYDwAAAAAA4KCy6oCntfb5JB9N8sgkL5t/vKo2Jzk6yW1JHrIjZy/+YWy/qao2LNDnWWN78/KqBQAAAAAA6N8kdvAkyVvH9tyqOn7mzap6YpJ3jC/f1lp7cM6xs6rqxqq6dN5Y12UIeQ5L8ntVdficcw6pqjdmCHjuz/CsHgAAAAAAgIeVSTyDJ621y6vqwiRnJrmhqj6Y5L4kpyU5PMl7kpw/77Qjk5yYYWfP3LHuraozkvxZkpck2VxV1ye5J8mmJMcleTDJ61prn5lE/QAAAAAAAD2ZSMCTJK21V1XVNUlenWRzkkOT3JjkoiQXzt29s4Sxrqiqb0vyk0mel+SUcbzbkvxRkre31v56UrUDAAAAAAD0ZGIBT5K01t6V5F1L7HtOknMWOX5Thh1BAAAAAAAAzDGpZ/AAAAAAAABwgAh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADqzbtoFcGBsuHj7AZlnx5aNB2QeAAAAAAB4OLODBwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOrNu2gVwcNlw8fYDNteOLRsP2FwAAAAAALCW2MEDAAAAAADQGQEPAAAAAABAZyYa8FTVK6rq6qraWVW7quojVfXqqlrRPFV1aFX9eFV9qKr+qaq+UlWfr6r3VdX3TbJ2AAAAAACAXkzsGTxVdUGSVyX5SpK/SnJfktOSnJ/ktKp6WWvtgWWM9/gk70/yHUl2JvlwkruSPHkc9/Yk75tU/QAAAAAAAL2YSMBTVadnCHduS3Jya+2m8f2vS3JlkhcnOSvJ25c43iEZwpvvSPK7SV7XWrt7zvH1SY6dRO0AAAAAAAC9mdQt2s4e29fPhDtJ0lq7PcmZ48s3LONWbT+a5DlJrkryY3PDnXHcXa21v1tlzQAAAAAAAF1adcBTVUcneXqSe5NcNv94a+2qJNuTHJXkWUsc9qyxPbe11lZbIwAAAAAAwMFkErdoe9rYfrK1ds8Cfa5PsnHse+1ig1XVUUm+OcMzfK6sqm9JcnqSr0/yT0mubK1dMYG6AQAAAAAAujSJgOe4sb1lkT6fm9d3Md86ttuS/HyG27/VnONnV9WHkpzeWrtjGXUCAAAAAAAcFCYR8Kwf27sX6bNrbB+7hPEeP7bHJfm5JJcmeWuG27w9I8kFSU5O8u4kz1tKgVV1RpIzltJ369atmzZt2rSUrkzZTTfdtO9OrJqfM8yyHmCW9QCzrAcYWAswy3qAWdYDJBs3btwv404i4JnZXTOpZ+XMPBdoXZK/aq29cs6xK6vqu5P83ySnVtXm8Rk/+3Jsks1LmXzXrl377gQAAAAAADBFkwh47hrb9Yv0mTl21yJ95o+XJL89/2Br7QtV9edJXprktCRLCXi2LbFf1q9fvynJEUvpy3SdcMIJ0y7hoDbzryv8nMF6gLmsB5hlPcDAWoBZ1gPMsh5g1u7du/fLuJMIeLaN7TGL9HnyvL5LGS9Jbl6gz8z7Ry1hvLTWLklyyVL67ty5c2uWuNsHAAAAAABgGg7Zd5d9+tjYnlRVhy3Q55nz+i7mxsw+z+cJC/Q5cmzdTw0AAAAAAHjYWXXA01r7fJKPJnlkkpfNP15Vm5McneS2JNctYbz7kvyP8eVpexnvEUlOHl9+ZGVVAwAAAAAA9GsSO3iS5K1je25VHT/zZlU9Mck7xpdva609OOfYWVV1Y1VdusB4DyZ5dVWdNuecQ5Ocm+QbkmxP8qcTqh8AAAAAAKAbk3gGT1prl1fVhUnOTHJDVX0wyX0ZduAcnuQ9Sc6fd9qRSU7MsLNn/nifqKrXJXl7kr+squuTfCHJ05I8NcnOJC9rrd0zifoBAAAAAAB6MqkdPGmtvSrJv89wu7bNSb4nyaeTnJXk9NbaA8sc7zeSPC/J+5Mcn+RfZwikfjvJptbaPm/3BgAAAAAAcDCayA6eGa21dyV51xL7npPknH302Zpk6yrLAgAAAAAAOKhMbAcPAAAAAAAAB4aABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDPrpl0ArNSGi7cfsLl2bNl4wOYCAAAAAIB9sYMHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADozLppFwA92HDx9gMyz44tGw/IPAAAAAAA9M0OHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6M9GAp6peUVVXV9XOqtpVVR+pqldX1arnqaofq6o2fp0/iXoBAAAAAAB6NLGAp6ouSPKHSZ6R5OokVyT5xiTnJ7m8qg5dxdjHJDkvSZtAqQAgtxacAAAe+ElEQVQAAAAAAF1bN4lBqur0JK9KcluSk1trN43vf12SK5O8OMlZSd6+grErye9lCKMuTfLKSdQMa9GGi7cfsLl2bNl4wOYCAAAAAGCyJrWD5+yxff1MuJMkrbXbk5w5vnzDCm/V9h+TnDbOsW01RQIAAAAAABwMJvFsnKOTPD3JvUkum3+8tXZVku1JjkryrGWOfVySX0ny4Qy3egMAAAAAAHjYm8QOnqeN7Sdba/cs0Of6eX33abw120UZbiP3I601z98BAAAAAADIZJ7Bc9zY3rJIn8/N67sUZyU5JckbWmufWkFdAAAAAAAAB6VJBDzrx/buRfrsGtvHLmXAqvqGJG9N8r+TnLfy0r463hlJzlhK361bt27atGnTaqeENe+mm27ad6dl9IOHA+sBZlkPMMt6gIG1ALOsB5hlPUCycePG/TLuJAKeGtuJ3EJtzq3ZHpnkh1trD0xg2GOTbF5Kx127du27EwAAAAAAwBRNIuC5a2zXL9Jn5thdi/SZ8ZokJyf5pdba366msDm2JblqKR3Xr1+/KckRE5oX1qwTTjhh0eMz/7piX/3g4cB6gFnWA8yyHmBgLcAs6wFmWQ8wa/fu3ftl3EkEPNvG9phF+jx5Xt/FvHhsX1BV83fdHDvTp6q+Ocmu1toL9zVga+2SJJcsYe7s3Llza5a42wcAAAAAAGAaJhHwfGxsT6qqw1pr9+ylzzPn9V2KZy9y7Enj185ljAcAAAAAAHBQOGS1A7TWPp/koxmemfOy+cfHXThHJ7ktyXVLGO+U1lrt7SvJm8ZuF4zvbVht/QAAAAAAAL1ZdcAzeuvYnltVx8+8WVVPTPKO8eXbWmsPzjl2VlXdWFWXTqgGAAAAAACAh4VJ3KItrbXLq+rCJGcmuaGqPpjkviSnJTk8yXuSnD/vtCOTnJhhZw8AAAAAAABLNJGAJ0laa6+qqmuSvDrJ5iSHJrkxyUVJLpy7ewcAAAAAAICVm1jAkySttXcledcS+56T5Jxljr/scwAAAAAAAA42k3oGDwAAAAAAAAeIgAcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM6sm3YBwHRsuHj7Pno8emiu2Ve/xe3YsnFV5wMAAAAA8FB28AAAAAAAAHRGwAMAAAAAANAZAQ8AAAAAAEBnBDwAAAAAAACdEfAAAAAAAAB0RsADAAAAAADQGQEPAAAAAABAZwQ8AAAAAAAAnRHwAAAAAAAAdEbAAwAAAAAA0BkBDwAAAAAAQGcEPAAAAAAAAJ0R8AAAAAAAAHRGwAMAAAAAANAZAQ8AAAAAAEBnBDwAAAAAAACdEfAAAAAAAAB0RsADAAAAAADQGQEPAAAAAABAZwQ8AAAAAAAAnRHwAAAAAAAAdEbAAwAAAAAA0BkBDwAAAAAAQGcEPAAAAAAAAJ0R8AAAAAAAAHRGwAMAAAAAANAZAQ8AAAAAAEBnBDwAAAAAAACdEfAAAAAAAAB0RsADAAAAAADQmXXTLgA4uG24ePsBm2vHlo0HbC4AAAAAgGmygwcAAAAAAKAzAh4AAAAAAIDOCHgAAAAAAAA6I+ABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDPrpl0AwKRsuHj7AZtrx5aNB2wuAAAAAID57OABAAAAAADojIAHAAAAAACgMwIeAAAAAACAzgh4AAAAAAAAOiPgAQAAAAAA6IyABwAAAAAAoDMCHgAAAAAAgM4IeAAAAAAAADoj4AEAAAAAAOiMgAcAAAAA+H/t3XuQZmddJ/DvLxkSyA4wAkF0IvfIKiWGq1yUoCkFXFZBCFLKLhlda00CXpBr7UKhqAQFFIGEXSUzUMqWIiteWFFBkjUCLsHAYiDrCAJxloSbk2Vym5j89o9zxh7aufTQ77xnzvTnU9X19Dnn6ef80plT3f1+3+d5AJgZAQ8AAAAAAMDMCHgAAAAAAABmRsADAAAAAAAwMwIeAAAAAACAmRHwAAAAAAAAzIyABwAAAAAAYGYEPAAAAAAAADMj4AEAAAAAAJgZAQ8AAAAAAMDMCHgAAAAAAABmRsADAAAAAAAwMwIeAAAAAACAmRHwAAAAAAAAzIyABwAAAAAAYGYEPAAAAAAAADMj4AEAAAAAAJgZAQ8AAAAAAMDMCHgAAAAAAABmZtPUBQDM0Zbtu5Zyn93bti7lPgAAAADAvJjBAwAAAAAAMDMCHgAAAAAAgJkR8AAAAAAAAMyMgAcAAAAAAGBmBDwAAAAAAAAzI+ABAAAAAACYGQEPAAAAAADAzAh4AAAAAAAAZkbAAwAAAAAAMDMCHgAAAAAAgJkR8AAAAAAAAMzMQgOeqvqhqvqLqrquqvZU1eVVdX5Vrfk+VXVCVT26qn5+HOsfqmpvVV1bVf+jqp68yJoBAAAAAADmZtOiBqqqNyQ5L8lNSd6T5JYkZyV5fZKzqurs7r51DUPdN8lfjp9/KcnlSS4bzz8xyROrakeSH+nuXlT9AMeiLdt3Le1eu7dtXdq9AAAAAID1WcgMnqp6aoZw55okD+ruJ3X3U5KcnuTjSZ6S5NlrHK6T/HmGMOfu3f347n5Gdz8iyeOSXJ/knPEDAAAAAABgw1nUEm0vHtsXdvfOfSe7+9ok546HL1rLUm3d/YnuPqu737V6xk93X5rkgvHwmQuoGwAAAAAAYHbWHfBU1WlJHppkb5K3rb4+hjK7ktwjySPXe78kV4ztaQsYCwAAAAAAYHYWMYPnwWN7ZXffeJA+H1zVdz1OH9vPLmAsAAAAAACA2VlEwHOfsf30Ifp8ZlXfr0pVnZLkJ8bDt69nLAAAAAAAgLnatIAxNo/t9Yfos2ds77jOe12YIST6WJL/utYvqqpzkpyzlr6XXHLJGWecccZXUxvArO3cufPwnTY43yNY4XmAFZ4HGHgWYIXnAVZ4HiDZunXrURl3EQFPjW0vYKyD36TqJUmeleS6JE/v7puP4MvvneTMtXTcs2fP4TsBAAAAAABMaBEBz5fHdvMh+uy79uVD9Dmoqnpukp/LMBPoid195REO8akkl66l4+bNm89IcucjHB9g9k4//fTDd9qg9r3byPcIPA+wP88DDDwLsMLzACs8D7DihhtuOCrjLiLg+dTY3usQfb5hVd81q6rnJHl1khuTPKm733+kY3T3jiQ71tL3uuuuuyRrnO0DAAAAAAAwhRMWMMYVY/vAqrrDQfo8fFXfNamq85P8WpKbknxfd69pFg4AAAAAAMDxbN0BT3dfneSvk5yU5OzV16vqzCSnJbkmyZpn31TVjyd5XZKbkzy5u9+93loBAAAAAACOB4uYwZMkrxjbV1bV/fedrKq7J7lwPLygu2/b79qzq+qqqnrL6sGq6sfGr9ub5Ae6+08WVCcAAAAAAMDsLWIPnnT371bVRUnOTfLRqnp3kluSnJXkTknekeT1q77sbkkekGFmzz+rqjOS/JckleTvkzy9qp5+gNt+obuft4j6AQAAAAAA5mQhAU+SdPd5VXVZkvOTnJnkxCRXJbk4yUX7z945jC0Zwp0k+dfjx4F8OomABwAAAAAA2HAWFvAkSXe/Nclb19j3ZUledoDzl2Ql4AEAAAAAAGCVRe3BAwAAAAAAwJIIeAAAAAAAAGZmoUu0ATBfW7bvWsp9dm/bupT7AAAAAMDxzAweAAAAAACAmRHwAAAAAAAAzIyABwAAAAAAYGYEPAAAAAAAADMj4AEAAAAAAJgZAQ8AAAAAAMDMCHgAAAAAAABmRsADAAAAAAAwMwIeAAAAAACAmRHwAAAAAAAAzIyABwAAAAAAYGY2TV0AABvLlu27lnav3du2Lu1eAAAAALBMZvAAAAAAAADMjIAHAAAAAABgZgQ8AAAAAAAAMyPgAQAAAAAAmBkBDwAAAAAAwMwIeAAAAAAAAGZGwAMAAAAAADAzAh4AAAAAAICZEfAAAAAAAADMjIAHAAAAAABgZjZNXQAAHC1btu9a0EinDM1lBx5v97atC7oPAAAAAKyNGTwAAAAAAAAzI+ABAAAAAACYGQEPAAAAAADAzAh4AAAAAAAAZkbAAwAAAAAAMDMCHgAAAAAAgJkR8AAAAAAAAMyMgAcAAAAAAGBmBDwAAAAAAAAzI+ABAAAAAACYmU1TFwAAc7dl+66l3Wv3tq1LuxcAAAAAxy4zeAAAAAAAAGZGwAMAAAAAADAzAh4AAAAAAICZEfAAAAAAAADMjIAHAAAAAABgZgQ8AAAAAAAAMyPgAQAAAAAAmBkBDwAAAAAAwMwIeAAAAAAAAGZGwAMAAAAAADAzAh4AAAAAAICZEfAAAAAAAADMzKapCwAA1m7L9l1Lu9fubVuXdi8AAAAAjowZPAAAAAAAADMj4AEAAAAAAJgZS7QBAAe0rOXgLAUHAAAAcOTM4AEAAAAAAJgZAQ8AAAAAAMDMCHgAAAAAAABmxh48AMCklrXXT2K/HwAAAOD4YQYPAAAAAADAzAh4AAAAAAAAZkbAAwAAAAAAMDMCHgAAAAAAgJkR8AAAAAAAAMzMpqkLAABYli3bdy3lPru3bV3KfQAAAICNywweAAAAAACAmRHwAAAAAAAAzIyABwAAAAAAYGYEPAAAAAAAADOzaeoCAACYhy3bdyU5ZTi4bNdRu8/ubVuP2tgAAABwvBDwAAAs2BCELIcwBAAAADYmS7QBAAAAAADMjIAHAAAAAABgZizRBgAwY8tcDg4AAAA4dpjBAwAAAAAAMDMCHgAAAAAAgJkR8AAAAAAAAMyMgAcAAAAAAGBmBDwAAAAAAAAzI+ABAAAAAACYGQEPAAAAAADAzAh4AAAAAAAAZkbAAwAAAAAAMDObpi4AAAD2t2X7rqXda/e2rUu7FwAAACySgAcAAI4yoRUAAACLZok2AAAAAACAmRHwAAAAAAAAzIwl2gAAgK/Kspaes+wcAADAvyTgAQBgw1rm3jgAAACwSAIeAACA0ZGFfqcMzWVHHhSalQQAAKyXgAcAAI4jx+OspOPxvwkAAGC9Tpi6AAAAAAAAAI6MgAcAAAAAAGBmBDwAAAAAAAAzYw8eAACA49iy9jDavW3rUu4DAAAMzOABAAAAAACYmYXO4KmqH0pybpIHJTkxyVVJtie5qLtv+yrGe0KS5yZ5WJLbJ/lkkv+W5FXdffOi6gYAAFimZc2qAQAAjl8LC3iq6g1JzktyU5L3JLklyVlJXp/krKo6u7tvPYLxXpDklUluTXJJkn9McmaSn0/ypKo6q7tvWFT9AAAAfPWWGVodj8vB+f4BAHCkFhLwVNVTM4Q71yR5bHfvHM9/bZL3JnlKkmcnee0ax3tYkguS3JDku7r7r8bzm5O8M8ljk/xCkp9eRP0AAADMhzAEAAAWN4PnxWP7wn3hTpJ097VVdW6GGTgvqqrXrXGpthclqSSv3BfujOPtqaptSXYmOa+qfra7dy/ovwEAAAC+guX0AAA4Vq074Kmq05I8NMneJG9bfb27L62qXUm2JnlkkvcdZryTkjxxPPytA4z3yap6f5LHJPneJG9d138AAAAAbCCLD61OGZrLvnJcs58AAI6uRczgefDYXtndNx6kzwczBDwPzmECniQPyPDb4Ze6+xOHGO8x43gCHgAAADjGmP00H8I4AJinRQQ89xnbTx+iz2dW9V3LeJ85RJ8jGQ8AAACAiW2M0O/AM9o4MOEiwPosIuDZPLbXH6LPnrG94wTjparOSXLOWvru3LnzUaeeemq+5S63yx894W5r+RIAAACA2brsszcv5T5eZ2G1Zf3bYyKb75kkudb/Z2biIXe+9aiNffLJJ+/79P6LHHcRAU+NbS9grKMxXpLcO8mZa+l40kknJUnufPIJ+favO/kwvQEAAAAAAA5v7969d1nkeIsIeL48tpsP0WfftS8fos/RGi9JPpXk0rV0vPrqq789yYl79+7de+qpp75/jePDcefDH/7wGXv27Lnz5s2brzvjjDM+PHU9MCXPA6zwPMAKzwMMPAuwwvMAKzwPsOLzn//8o0466aSTPve5z9166qmnLmzc6l7fRJmq+r4kv5/kiu5+yEH6/PckT0nynO5+/WHGe1CSjyT5Unff9SB9XpPkp5O8uruft576DzD2JRlm+1za3Y9b5NgwJ54FWOF5gBWeB1jheYCBZwFWeB5ghecBVhyt5+GEBYxxxdg+sKrucJA+D1/V91CuSnJjkrtU1f0O0ucRRzAeAAAAAADAcWXdAU93X53kr5OclOTs1der6swkpyW5Jslhlzzr7r1J/ng8/OEDjHffJI9KsjfJO7/qwgEAAAAAAGZqETN4kuQVY/vKqrr/vpNVdfckF46HF3T3bftde3ZVXVVVbznAeBck6SQvrKpH7Pc1m5NcPNZ9YXfvXlD9AAAAAAAAs7GQgKe7fzfJRUnukeSjVfWH4747O5N8c5J3JFm9987dkjwgyT0PMN4Hk7woySlJ3ldVf1pVv5PkExnWqfurJP9pEbUDAAAAAADMzaZFDdTd51XVZUnOzxDCnJhhP52Lk1y0/+ydNY73S1X1v5P8TIY9fG6f5JNJfi3Jq7r75kXVDgAAAAAAMCcLC3iSpLvfmuSta+z7siQvO0yfdyV517oLAwAAAAAAOI4sag8eAAAAAAAAlkTAAwAAAAAAMDMCHgAAAAAAgJlZ6B48x4kdSS5J8qlJq4Dp7YhnAfbZEc8D7LMjngfYZ0c8D5B4FmB/O+J5gH12xPMA++zIUXgeqrsXOR4AAAAAAABHmSXaAAAAAAAAZkbAAwAAAAAAMDMCHgAAAAAAgJkR8AAAAAAAAMyMgAcAAAAAAGBmBDyjqvqhqvqLqrquqvZU1eVVdX5V+R6xYVTVA6rqJ6vqN6vqqqq6raq6qp42dW2wTFV1u6o6q6peXVUfqKrPVtXeqtpVVb9bVY+bukZYpqp6TlX9TlV9vKq+WFW3VNXnq+rdVfXMqqqpa4SpVNUvjr8vdVU9b+p6YFmqasd+//YP9HHV1DXCslXVHarqBVX1waraXVU3VNXfV9XbquoxU9cHR1tVPe4wPxv2/7jn1PXC0VZVp1XV66rq/1TVjVV1U1XtrKo3VtV9F3GPTYsYZO6q6g1JzktyU5L3JLklyVlJXp/krKo6u7tvnbBEWJZzk/zk1EXAMeDMJH82fn5Nkg8luT7JNyd5apKnVtXLu/ulE9UHy/bCJHdP8jdJ3pfhebhXku/K8DvT06rqB7r7tulKhOWrqocneUGSTiLoZKP6yyR/d4Dzn112ITClqrpPkj9Ncv8kn0tyaZKbk9w7yfcn+UiG5wWOZ9ckefMhrj8iyTcl+USSq5dSEUykqh6c5M+TbEnyD0n+ZLz0sCT/MckPV9Xju/t967nPhg94quqpGcKda5I8trt3jue/Nsl7kzwlybOTvHayImF5/ibJLye5PMML2m/K8EI3bDS3JXl7ktd291/sf6GqfjDJbyV5SVW9t7vfO0WBsGTPSHJFd1+//8mqemCGN8d8f5JnJdk+QW0wiao6OcmOJNcm+V9JnjxpQTCd3+juHVMXAVOqqn+V4Q1i90vy8iQv7+5b9rt+1yR3nag8WJruvirJOQe7XlVXjp9e3N29lKJgOm/IEO78epLz9/1cqKrbJXljkh9JclGSb13PTSw/lrx4bF+4L9xJku6+NsNshiR5kaXa2Ai6+ze6+wXd/Tvd/Ymp64GpdPefd/fTVoc747XfzvCCXpI8c6mFwUS6+7LV4c54/soMv7QmyXcvtyqY3M9lmNn540mum7gWAKb1nzOEO2/p7pfuH+4kSXd/sbv/dprS4NhQVY/K8LvTrTn0LB+Yvaq6fZJHjYdf8XNh/Pwl4+GDquqU9dxrQ4cWVXVakocm2Zvkbauvd/elSXYluUeSRy63OgCOYVeM7WmTVgHHhn8a25smrQKWqKq+LcnPJHlrd//h1PUAMJ2qOinJj42HF0xZCxzjfmRs39XduyatBI6+W7Pyt/KBlnLeN4Pt+iQ3rudGG32JtgeP7ZXdfbBv5AeTbB37rms9PACOG6ePrbXl2dDGteZ/fDz0IjcbwvhuvDcn+VLsXQhJ8p1V9aAkmzMsWXhZkj+zLxsbyEMzLL92dXd/vKoeneRJ47lrMryY/f4pC4SpjTMUfnA8fNOUtcAydPctVfWeJI9P8rNVtXqJtp8fu75pvcsVbvSA5z5j++lD9PnMqr4AbGBVdY+srCn89glLgaWrqm0Z9ma7XYYZbI/OMCP8Fd39e1PWBkv0C0kekOQZ3f2FqYuBY8C/P8C5j1XVM7r7o0uvBpbvW8Z2Z1XtyLAv4f5eWlVvT/LvDvHmYjjenZ3kjkk+l+SPJq4FluW8JO/KMMvziVV1+Xj+4Um+Jslrkzx/vTfZ6AHP5rH9F2vK72fP2N7xKNcCwDGuqjYl+c0kd07yHsvysAE9Jl/5osU/ZVg7+DXTlAPLNb4r+6eSvGPckw02sg8n+VCS92R40+SdkjwkQwj6rUneXVUPsQwPG8BdxvaxSU5M8qoMm2d/cTx3YZKnJvl/WVmiCjaaff/237J6jyo4XnX3J8e/H96S5In5ymX+L0/yPxfxPGzoPXiysv7duqZBAbBhvDHJWUmuTvLMiWuBpevu/9DdleSUJA9M8qtJXpbkA1X19VPWBkdbVd0hyfYML9CdN3E5MLnu/tXufl13f6y7r+/uz3b3O5M8IskHktw9yYunrRKWYt9ra5syLLXz/O7+RHfv7u4/SPLkDK87Pauq7jtZlTCRqrp/hrAzSS6eshZYpjHc+Zsk90/y/UnuluTUDD8XvibJ26vqpeu9z0YPeL48tpsP0WfftS8fog8Ax7mqem2SH82wjvZZ3X3NxCXBZLr7xvEFvednePHuW5O8fuKy4Gj7xSTfmOS53W0PNjiI7t6b5BXj4fdOWQssyf6vF/366ovdfXmG2W4nJHnckmqCY8m+2Tvv7+6PT1oJLElVbUnyjgyrgj2hu/+gu7/Y3V/o7t9P8oQkNyZ5SVWdfqixDmejBzyfGtt7HaLPN6zqC8AGU1WvTvITST6fIdzZOXFJcCzZPrb/dtwsEo5XT0lyW4Z3YF+y/0eGP9CS5Nzx3G9MViUcG64a262TVgHL8an9Pv/7g/TZd/4eR7cUOLZU1YlZ2avtTVPWAkv2bzLM1vlAd39y9cXu/rskf5Vh9ufj1nOjjb4HzxVj+8CqusNBNrt7+Kq+AGwgVfVLSZ6bYQ3t7+7uj01cEhxrdmfYi2dThjXor522HDiqTkhy5iGu33f82LKccuCYddex3XPIXnB8+Ov9Pr9rhjeFrXa3sfVMsNE8PkPYf30S+xeykdxzbK87RJ/dY3uXQ/Q5rA09g6e7r87wg/ikJGevvl5VZ2bY/OiaJO9fbnUATK2qLkjy/CT/mCHc+cjEJcGx6LEZwp3dSb4wcS1w1HT3vbu7DvSR5M1jt+eP586YslY4Bjx9bD84aRWwBN29K8O7sJNhv86vUFVfk+Qh4+Hly6oLjhE/Ora/3d0CTjaS/zu2Dz3QShfjuYeOhweb/bkmGzrgGe1bG/iV46ZfSZKqunuSC8fDC7r7tqVXBsBkqurlSV6Y4UXr7+5uMznZkKrqO6rqh6vq5ANce0xWllp4U3ffutzqAJhCVZ1RVU8al97Z//ymqnpuhqVtk+RXll8dTOIXxvalVfXPIX9V3T7JRUnunGEfHm8eZsOoqrsledJ4aHk2Npo/TnJDhpk8v7L/39Pj57+WYWuYf0zyJ+u5UXX3er7+uFBVFyY5N8lNSd6d5JYM77q4U4bNkJ7mBQs2gqp6SFaCzST55gybge1M8qV9J7v7kUsuDZaqqr4vye+Ph5cnufIgXa/q7guWUxVMo6rOybDPzu4MM5+vyfCz4X4Zfk4kyTuTnH2Q5W7huFdVO5I8K8MMnldNXA4cdVX15CS/l+FvhL9N8g8ZfjZ8S5Kvz7Bf1Yu7+5cmKxKWrKp+OcnzkuzNMKPni0kekeGZ2JXkO+3lyUZSVT+d5DUZ/m7+pqnrgWWrqmdlCDdPzDCj50NJKsPMna9LcnOSZ3T3O9Zzn42+B0+SpLvPq6rLkpyfYU3tEzNsCnlxkovM3mEDuVOSbzvA+dOXXQhMbP/1Tx82fhzIpUkEPBzvLk3y8iTfkeQbkzw6wy+l1yR5e5LfXO8vpADMzkeSvDbDi9f3SvLgJJ0h6Nme5A3d/aHpyoPl6+7nV9X7kjwnwzNxSpLPZHiB+4LuPtDePHA82za2F09aBUyku99cVR9N8lMZ/p7+nvHSrgzBz2sWsc+zGTwAAAAAAAAzYw8eAAAAAACAmRHwAAAAAAAAzIyABwAAAAAAYGYEPAAAAAAAADMj4AEAAAAAAJgZAQ8AAAAAAMDMCHgAAAAAAABmRsADAAAAAAAwMwIeAAAAAACAmRHwAAAAAAAAzIyABwAAAAAAYGYEPAAAAAAAADMj4AEAAAAAAJgZAQ8AAAAAAMDMCHgAAAAAAABmRsADAAAAAAAwMwIeAAAAAACAmfn/FDUAdE/yktsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 900x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 322,
       "width": 828
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define our observed samples\n",
    "lambda_1 = tfd.Exponential(rate=1., name=\"lambda_1\")\n",
    "samples = lambda_1.sample(sample_shape=20000)\n",
    "    \n",
    "# Execute graph, convert TF to NumPy\n",
    "[ samples_ ] = evaluate([ samples ])\n",
    "\n",
    "# Visualize our stepwise prior distribution\n",
    "plt.figure(figsize(12.5, 5))\n",
    "plt.hist(samples_, bins=70, density=True, histtype=\"stepfilled\")\n",
    "plt.title(r\"Prior distribution for $\\lambda_1$\")\n",
    "plt.xlim(0, 8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9nOs9Gq3IAxH"
   },
   "source": [
    "To frame this in the notation of the first chapter, though this is a slight abuse of notation, we have specified $P(A)$. Our next goal is to include data/evidence/observations $X$ into our model. \n",
    "\n",
    "Sometimes we may want to match a property of our distribution to a property of observed data. To do so, we get the parameters for our distribution fom the data itself. In this example, the Poisson rate (average number of events) is explicitly set to one over the average of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T15:43:09.480725Z",
     "start_time": "2019-01-12T15:43:09.420684Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "qtHXSR6QIAxH",
    "outputId": "cad9361a-f6ee-4750-8e24-b46656babcb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two predetermined data points:  [10.  5.]\n",
      "\n",
      " mean of our data:  7.5\n",
      "\n",
      " random sample from poisson distribution \n",
      " with the mean as the poisson's rate: \n",
      " 0.0\n"
     ]
    }
   ],
   "source": [
    "data = tf.constant([10., 5.], dtype=tf.float32)\n",
    "poisson = tfd.Poisson(rate=1./tf.reduce_mean(data))\n",
    "\n",
    "\n",
    "# Execute graph\n",
    "[ data_, poisson_sample_, ] = evaluate([ data, poisson.sample() ])\n",
    "\n",
    "print(\"two predetermined data points: \", data_)\n",
    "print(\"\\n mean of our data: \", np.mean(data_))\n",
    "\n",
    "\n",
    "print(\"\\n random sample from poisson distribution \\n with the mean as the poisson's rate: \\n\", poisson_sample_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8oxo5VcbIAxP"
   },
   "source": [
    "## Modeling approaches\n",
    "\n",
    "A good starting thought to Bayesian modeling is to think about *how your data might have been generated*. Position yourself in an omniscient position, and try to imagine how *you* would recreate the dataset. \n",
    "\n",
    "In the last chapter we investigated text message data. We begin by asking how our observations may have been generated:\n",
    "\n",
    "1.  We started by thinking \"what is the best random variable to describe this count data?\" A Poisson random variable is a good candidate because it can represent count data. So we model the number of sms's received as sampled from a Poisson distribution.\n",
    "\n",
    "2.  Next, we think, \"Ok, assuming sms's are Poisson-distributed, what do I need for the Poisson distribution?\" Well, the Poisson distribution has a parameter $\\lambda$. \n",
    "\n",
    "3.  Do we know $\\lambda$? No. In fact, we have a suspicion that there are *two* $\\lambda$ values, one for the earlier behaviour and one for the later behaviour. We don't know when the behaviour switches though, but call the switchpoint $\\tau$.\n",
    "\n",
    "4. What is a good distribution for the two $\\lambda$s? The exponential is good, as it assigns probabilities to positive real numbers. Well the exponential distribution has a parameter too, call it $\\alpha$.\n",
    "\n",
    "5.  Do we know what the parameter $\\alpha$ might be? No. At this point, we could continue and assign a distribution to $\\alpha$, but it's better to stop once we reach a set level of ignorance: whereas we have a prior belief about $\\lambda$, (\"it probably changes over time\", \"it's likely between 10 and 30\", etc.), we don't really have any strong beliefs about $\\alpha$. So it's best to stop here. \n",
    "\n",
    "    What is a good value for $\\alpha$ then? We think that the $\\lambda$s are between 10-30, so if we set $\\alpha$ really low (which corresponds to larger probability on high values) we are not reflecting our prior well. Similar, a too-high alpha misses our prior belief as well. A good idea for $\\alpha$ as to reflect our belief is to set the value so that the mean of $\\lambda$, given $\\alpha$, is equal to our observed mean. This was shown in the last chapter.\n",
    "\n",
    "6. We have no expert opinion of when $\\tau$ might have occurred. So we will suppose $\\tau$ is from a discrete uniform distribution over the entire timespan.\n",
    "\n",
    "\n",
    "Below we give a graphical visualization of this, where arrows denote `parent-child` relationships. (provided by the [Daft Python library](http://daft-pgm.org/) )\n",
    "\n",
    "<img src=\"http://i.imgur.com/7J30oCG.png\">\n",
    "\n",
    "\n",
    "TFP and other probabilistic programming languages have been designed to tell these data-generation *stories*. More generally, B. Cronin writes [2]:\n",
    "\n",
    "> Probabilistic programming will unlock narrative explanations of data, one of the holy grails of business analytics and the unsung hero of scientific persuasion. People think in terms of stories - thus the unreasonable power of the anecdote to drive decision-making, well-founded or not. But existing analytics largely fails to provide this kind of story; instead, numbers seemingly appear out of thin air, with little of the causal context that humans prefer when weighing their options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3RJEK_yjIAxR"
   },
   "source": [
    "### Same story; different ending.\n",
    "\n",
    "Interestingly, we can create *new datasets* by retelling the story.\n",
    "For example, if we reverse the above steps, we can simulate a possible realization of the dataset.\n",
    "\n",
    "1\\. Specify when the user's behaviour switches by sampling from $\\text{DiscreteUniform}(0, 80)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T15:43:09.521001Z",
     "start_time": "2019-01-12T15:43:09.480725Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ma56S7r1IAxS",
    "outputId": "68ec7a14-3090-472f-dfcf-0bf6445a233c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of Tau (randomly taken from DiscreteUniform(0, 80)): 0\n"
     ]
    }
   ],
   "source": [
    "tau = tf.random_uniform(shape=[1], minval=0, maxval=80, dtype=tf.int32)[0]\n",
    "\n",
    "[ tau_ ] = evaluate([ tau ])\n",
    "\n",
    "print(\"Value of Tau (randomly taken from DiscreteUniform(0, 80)):\", tau_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xt_6sYG6IAxW"
   },
   "source": [
    "2\\. Draw $\\lambda_1$ and $\\lambda_2$ from a $\\text{Gamma}(\\alpha)$ distribution:\n",
    "\n",
    "Note: A gamma distribution is a generalization of the exponential distribution. A gamma distribution with shape parameter $ = 1$ and scale parameter $$ is an  exponential ($$) distribution. Here, we use a gamma distribution to have more flexibility than we would have had were we to model with an exponential. Rather than returning values between $0$ and $1$, we can return values much larger than $1$ (i.e., the kinds of numbers one would expect to show up in a daily SMS count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T15:43:09.600912Z",
     "start_time": "2019-01-12T15:43:09.521001Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "l2QX3nEbofZr",
    "outputId": "1c78f1b5-4847-49b0-a9a6-2155285e5d03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda 1 (randomly taken from Gamma() distribution):  25.366709\n",
      "Lambda 2 (randomly taken from Gamma() distribution):  19.867706\n"
     ]
    }
   ],
   "source": [
    "alpha = 1./8.\n",
    "\n",
    "lambdas  = tfd.Gamma(concentration=1/alpha, rate=0.3).sample(sample_shape=[2])  \n",
    "[ lambda_1_, lambda_2_ ] = evaluate( lambdas )\n",
    "print(\"Lambda 1 (randomly taken from Gamma() distribution): \", lambda_1_)\n",
    "print(\"Lambda 2 (randomly taken from Gamma() distribution): \", lambda_2_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uIoKaO4bIAxb"
   },
   "source": [
    "3\\.  For days before $\\tau$, represent the user's received SMS count by sampling from $\\text{Poi}(\\lambda_1)$, and sample from  $\\text{Poi}(\\lambda_2)$ for days after $\\tau$. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T15:43:09.721061Z",
     "start_time": "2019-01-12T15:43:09.603973Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "6xxOtwxvpk_P",
    "outputId": "83a9c98c-df27-411a-b9b2-c59e1f33765f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial day-by-day user SMS count created by sampling: \n",
      " [23. 21. 17. 27. 18. 22. 17. 22. 31. 12. 20. 19. 18. 17. 28. 18. 25. 19.\n",
      " 22. 29. 22. 13. 21. 18. 18. 17. 18. 13. 13. 12. 20. 14. 20. 15. 28. 10.\n",
      " 25. 25. 18. 31. 23. 25. 16. 24. 18. 23. 21. 15. 25. 22. 22. 13. 26. 26.\n",
      " 15. 15. 19. 24. 20. 15. 23. 31. 25. 23. 20. 20. 24. 33. 28. 28. 21. 15.\n",
      " 26. 23. 15. 24. 17. 17. 26. 17.]\n"
     ]
    }
   ],
   "source": [
    "data = tf.concat([tfd.Poisson(rate=lambda_1_).sample(sample_shape=tau_),\n",
    "                      tfd.Poisson(rate=lambda_2_).sample(sample_shape= (80 - tau_))], axis=0)\n",
    "days_range = tf.range(80)\n",
    "[ data_, days_range_ ] = evaluate([ data, days_range ])\n",
    "print(\"Artificial day-by-day user SMS count created by sampling: \\n\", data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5PWuas1oIAxg"
   },
   "source": [
    "4\\. Plot the artificial dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T15:43:10.080813Z",
     "start_time": "2019-01-12T15:43:09.723565Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "colab_type": "code",
    "id": "vrGXdyZyIAxh",
    "outputId": "e4987d94-1fa3-44c3-c7f5-edd18c727c01"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 337,
       "width": 843
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(days_range_, data_, color=TFColor[3])\n",
    "plt.bar(tau_ - 1, data_[tau_ - 1], color=\"r\", label=\"user behaviour changed\")\n",
    "plt.xlabel(\"Time (days)\")\n",
    "plt.ylabel(\"count of text-msgs received\")\n",
    "plt.title(\"Artificial dataset\")\n",
    "plt.xlim(0, 80)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ulb46iQIIAxn"
   },
   "source": [
    "It is okay that our fictional dataset does not look like our observed dataset: the probability is incredibly small it indeed would. TFP's engine is designed to find good parameters, $\\lambda_i, \\tau$, that maximize this probability.  \n",
    "\n",
    "\n",
    "The ability to generate an artificial dataset is an interesting side effect of our modeling, and we will see that this ability is a very important method of Bayesian inference. We produce a few more datasets below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T15:43:11.570973Z",
     "start_time": "2019-01-12T15:43:10.080813Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "vrLFYvVzIAxp",
    "outputId": "1feb58e1-242f-433f-8cd4-035a588ae7ea"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 308,
       "width": 827
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_artificial_sms_dataset():   \n",
    "    tau = tf.random_uniform(shape=[1], \n",
    "                            minval=0, \n",
    "                            maxval=80,\n",
    "                            dtype=tf.int32)[0]\n",
    "    alpha = 1./8.\n",
    "    lambdas  = tfd.Gamma(concentration=1/alpha, rate=0.3).sample(sample_shape=[2]) \n",
    "    [ lambda_1_, lambda_2_ ] = evaluate( lambdas )\n",
    "    data = tf.concat([tfd.Poisson(rate=lambda_1_).sample(sample_shape=tau),\n",
    "                      tfd.Poisson(rate=lambda_2_).sample(sample_shape= (80 - tau))], axis=0)\n",
    "    days_range = tf.range(80)\n",
    "    \n",
    "    [ \n",
    "        tau_,\n",
    "        data_,\n",
    "        days_range_,\n",
    "    ] = evaluate([ \n",
    "        tau,\n",
    "        data,\n",
    "        days_range,\n",
    "    ])\n",
    "    \n",
    "    plt.bar(days_range_, data_, color=TFColor[3])\n",
    "    plt.bar(tau_ - 1, data_[tau_ - 1], color=\"r\", label=\"user behaviour changed\")\n",
    "    plt.xlim(0, 80);\n",
    "\n",
    "\n",
    "plt.figure(figsize(12.5, 5))\n",
    "plt.title(\"More example of artificial datasets\")\n",
    "for i in range(4):\n",
    "    plt.subplot(4, 1, i+1)\n",
    "    plot_artificial_sms_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8QMiJXOIAxv"
   },
   "source": [
    "Later we will see how we use this to make predictions and test the appropriateness of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2lU8C4C-IAxw"
   },
   "source": [
    "### Example: Bayesian A/B testing\n",
    "\n",
    "A/B testing is a statistical design pattern for determining the difference of effectiveness between two different treatments. For example, a pharmaceutical company is interested in the effectiveness of drug A vs drug B. The company will test drug A on some fraction of their trials, and drug B on the other fraction (this fraction is often 1/2, but we will relax this assumption). After performing enough trials, the in-house statisticians sift through the data to determine which drug yielded better results. \n",
    "\n",
    "Similarly, front-end web developers are interested in which design of their website yields more sales or some other metric of interest. They will route some fraction of visitors to site A, and the other fraction to site B, and record if the visit yielded a sale or not. The data is recorded (in real-time), and analyzed afterwards. \n",
    "\n",
    "Often, the post-experiment analysis is done using something called a hypothesis test like *difference of means test* or *difference of proportions test*. This involves often misunderstood quantities like a \"Z-score\" and even more confusing \"p-values\" (please don't ask). If you have taken a statistics course, you have probably been taught this technique (though not necessarily *learned* this technique). And if you were like me, you may have felt uncomfortable with their derivation -- good: the Bayesian approach to this problem is much more natural. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AFcmkQEyDgyK"
   },
   "source": [
    "### A Simple Case\n",
    "\n",
    "As this is a hacker book, we'll continue with the web-dev example. For the moment, we will focus on the analysis of site A only. Assume that there is some true $0 \\lt p_A \\lt 1$ probability that users who, upon shown site A, eventually purchase from the site. This is the true effectiveness of site A. Currently, this quantity is unknown to us. \n",
    "\n",
    "Suppose site A was shown to $N$ people, and $n$ people purchased from the site. One might conclude hastily that $p_A = \\frac{n}{N}$. Unfortunately, the *observed frequency* $\\frac{n}{N}$ does not necessarily equal $p_A$ -- there is a difference between the *observed frequency* and the *true frequency* of an event. The true frequency can be interpreted as the probability of an event occurring. For example, the true frequency of rolling a 1 on a 6-sided die is $\\frac{1}{6}$. Knowing the true frequency of events like:\n",
    "\n",
    "- fraction of users who make purchases, \n",
    "- frequency of social attributes, \n",
    "- percent of internet users with cats etc. \n",
    "\n",
    "are common requests we ask of Nature. Unfortunately, often Nature hides the true frequency from us and we must *infer* it from observed data.\n",
    "\n",
    "The *observed frequency* is then the frequency we observe: say rolling the die 100 times you may observe 20 rolls of 1. The observed frequency, 0.2, differs from the true frequency, $\\frac{1}{6}$. We can use Bayesian statistics to infer probable values of the true frequency using an appropriate prior and observed data.\n",
    "\n",
    "\n",
    "With respect to our A/B example, we are interested in using what we know, $N$ (the total trials administered) and $n$ (the number of conversions), to estimate what $p_A$, the true frequency of buyers, might be. \n",
    "\n",
    "To setup a Bayesian model, we need to assign prior distributions to our unknown quantities. *A priori*, what do we think $p_A$ might be? For this example, we have no strong conviction about $p_A$, so for now, let's assume $p_A$ is uniform over $[0,1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T15:43:11.610896Z",
     "start_time": "2019-01-12T15:43:11.570973Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "blTLKyo2IAxy"
   },
   "outputs": [],
   "source": [
    "reset_sess()\n",
    "\n",
    "# The parameters are the bounds of the Uniform.\n",
    "p = tfd.Uniform(low=0., high=1., name='p')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f0XLF9h3IAx2"
   },
   "source": [
    "Had we had stronger beliefs, we could have expressed them in the prior above.\n",
    "\n",
    "For this example, consider $p_A = 0.05$, and $N = 1500$ users shown site A, and we will simulate whether the user made a purchase or not. To simulate this from $N$ trials, we will use a *Bernoulli* distribution: if  $X\\ \\sim \\text{Ber}(p)$, then $X$ is 1 with probability $p$ and 0 with probability $1 - p$. Of course, in practice we do not know $p_A$, but we will use it here to simulate the data. We can assume then that we can use the following generative model:\n",
    "\n",
    "$$\\begin{align*}\n",
    "p &\\sim \\text{Uniform}[\\text{low}=0,\\text{high}=1) \\\\\n",
    "X\\ &\\sim \\text{Bernoulli}(\\text{prob}=p) \\\\\n",
    "\\text{for }  i &= 1\\ldots N:\\text{# Users}  \\\\\n",
    " X_i\\ &\\sim \\text{Bernoulli}(p_i)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T15:43:11.731008Z",
     "start_time": "2019-01-12T15:43:11.615413Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "riLrk5KTIAx4",
    "outputId": "6c8cf3ef-cdef-4340-f941-9f6ea1c7454e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of 1500 Occurences: [0 0 0 ... 0 0 0]\n",
      "(Remember: Python treats True == 1, and False == 0)\n",
      "Sum of (True == 1) Occurences: 76\n"
     ]
    }
   ],
   "source": [
    "reset_sess()\n",
    "\n",
    "#set constants\n",
    "prob_true = 0.05  # remember, this is unknown.\n",
    "N = 1500\n",
    "\n",
    "# sample N Bernoulli random variables from Ber(0.05).\n",
    "# each random variable has a 0.05 chance of being a 1.\n",
    "# this is the data-generation step\n",
    "\n",
    "occurrences = tfd.Bernoulli(probs=prob_true).sample(sample_shape=N, seed=6.45)\n",
    "\n",
    "[ \n",
    "    occurrences_,\n",
    "    occurrences_sum_,\n",
    "    occurrences_mean_,\n",
    "] = evaluate([ \n",
    "    occurrences, \n",
    "    tf.reduce_sum(occurrences),\n",
    "    tf.reduce_mean(tf.to_float(occurrences))\n",
    "])\n",
    "\n",
    "print(\"Array of {} Occurences:\".format(N), occurrences_) \n",
    "print(\"(Remember: Python treats True == 1, and False == 0)\")\n",
    "print(\"Sum of (True == 1) Occurences:\", occurrences_sum_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UpJrMifMIAx7"
   },
   "source": [
    "The observed frequency is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.605Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "trjtemdNIAx7",
    "outputId": "d528af9d-8d7c-49bc-c191-ca58fca5b6bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the observed frequency in Group A? 0.0507\n",
      "Does this equal the true frequency? False\n"
     ]
    }
   ],
   "source": [
    "# Occurrences.mean is equal to n/N.\n",
    "print(\"What is the observed frequency in Group A? %.4f\" % occurrences_mean_)\n",
    "print(\"Does this equal the true frequency? %s\" % (occurrences_mean_ == prob_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gue-SRTYIAyA"
   },
   "source": [
    "We combine our Bernoulli distribution and our observed occurrences into a log probability function based on the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.615Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Ct9o0w7lGaZb"
   },
   "outputs": [],
   "source": [
    "def joint_log_prob(occurrences, prob_A):\n",
    "    \"\"\"\n",
    "    Joint log probability optimization function.\n",
    "        \n",
    "    Args:\n",
    "      occurrences: An array of binary values (0 & 1), representing \n",
    "                   the observed frequency\n",
    "      prob_A: scalar estimate of the probability of a 1 appearing \n",
    "    Returns: \n",
    "      Joint log probability optimization function.\n",
    "    \"\"\"  \n",
    "    \n",
    "    rv_prob_A = tfd.Uniform(low=0., high=1.)\n",
    "  \n",
    "    rv_occurrences = tfd.Bernoulli(probs=prob_A)\n",
    "  \n",
    "    return (\n",
    "        rv_prob_A.log_prob(prob_A)\n",
    "        + tf.reduce_sum(rv_occurrences.log_prob(occurrences))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UN7Mh5U-uFye"
   },
   "source": [
    "The goal of probabilistic inference is to find model parameters that may explain\n",
    "data you have observed. TFP performs probabilistic inference by evaluating the\n",
    "model parameters using a `joint_log_prob` function.  The arguments to `joint_log_prob` are data and model parametersfor the model defined in the `joint_log_prob` function itself. The function returns the log of the joint probability that the model parameterized as such generated the observed data per the input arguments.\n",
    "\n",
    "All `joint_log_prob` functions have a common structure:\n",
    "\n",
    "1. The function takes a set of **inputs** to evaluate. Each input is either an\n",
    "observed value or a model parameter.\n",
    "\n",
    "1. The `joint_log_prob` function uses probability distributions to define a **model** for evaluating the inputs. These distributions measure the likelihood of the input values. (By convention, the distribution that measures the likelihood of the variable `foo` will be named `rv_foo` to note that it is a random variable.) We use two types of distributions in `joint_log_prob` functions:\n",
    "\n",
    "  a. **Prior distributions** measure the likelihood of input values.\n",
    "A prior distribution never depends on an input value each prior distribution measures the\n",
    "likelihood of a single input value. Each unknown variableone that has not been\n",
    "observed directlyneeds a corresponding prior. Beliefs about which values could\n",
    "be reasonable determine the prior distribution. Choosing a prior can be tricky,\n",
    "so we will cover it in depth in Chapter 6.\n",
    "\n",
    "  b. **Conditional distributions** measure the likelihood of an input value given\n",
    "other input values. Typically, the conditional\n",
    "distributions return the likelihood of observed data given the current guess of parameters in the model, p(observed_data | model_parameters).\n",
    "\n",
    "1. Finally, we calculate and return the **joint log probability** of the inputs.\n",
    "The joint log probability is the sum of the log probabilities from all of the\n",
    "prior and conditional distributions. (We take the sum of log probabilities\n",
    "instead of multiplying the probabilities directly for reasons of numerical\n",
    "stability: floating point numbers in computers cannot represent the very small\n",
    "values necessary to calculate the joint log probability unless they are in \n",
    "log space.) The sum of probabilities is actually an unnormalized density; although the total sum of probabilities  over all possible inputs might not sum to one, the sum of probabilities is proportional to the true probability density. This proportional distribution is sufficient to estimate the distribution of likely inputs.\n",
    "\n",
    "Let's map these terms onto the code above. In this example, the input values\n",
    "are the observed values in `occurrences` and the unknown value for `prob_A`. The `joint_log_prob` takes the current guess for `prob_A`\n",
    "and answers, how likely is the data if `prob_A` is the probability of\n",
    "`occurrences`. The answer depends on two distributions:\n",
    "1. The prior distribution, `rv_prob_A`, indicates how likely the current value of `prob_A` is by itself.\n",
    "2. The conditional distribution, `rv_occurrences`, indicates the likelihood of `occurrences` if `prob_A` were the  probability for the Bernoulli distribution.\n",
    "\n",
    "The sum of the log of these probabilities is the\n",
    "joint log probability. \n",
    "\n",
    "The `joint_log_prob` is particularly useful in conjunction with the [`tfp.mcmc`](https://www.tensorflow.org/probability/api_docs/python/tfp/mcmc)\n",
    "module. Markov chain Monte Carlo (MCMC) algorithms proceed by making educated guesses about the unknown\n",
    "input values and\n",
    "computing what the likelihood of this set of arguments is. (Well talk about how it makes those guesses in Chapter 3.) By repeating this process\n",
    "many times, MCMC builds a distribution of likely parameters. Constructing this\n",
    "distribution is the goal of probabilistic inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rzm3amOgDAGg"
   },
   "source": [
    "Then we run our inference algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.628Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "g9XHX0h8IAyB"
   },
   "outputs": [],
   "source": [
    "number_of_steps = 48000 #@param {type:\"slider\", min:2000, max:50000, step:100}\n",
    "#@markdown (Default is 18000).\n",
    "burnin = 25000 #@param {type:\"slider\", min:0, max:30000, step:100}\n",
    "#@markdown (Default is 1000).\n",
    "leapfrog_steps=2 #@param {type:\"slider\", min:1, max:9, step:1}\n",
    "#@markdown (Default is 6).\n",
    "\n",
    "# Set the chain's start state.\n",
    "initial_chain_state = [\n",
    "    tf.reduce_mean(tf.to_float(occurrences)) \n",
    "    * tf.ones([], dtype=tf.float32, name=\"init_prob_A\")\n",
    "]\n",
    "\n",
    "# Since HMC operates over unconstrained space, we need to transform the\n",
    "# samples so they live in real-space.\n",
    "unconstraining_bijectors = [\n",
    "    tfp.bijectors.Identity()   # Maps R to R.  \n",
    "]\n",
    "\n",
    "# Define a closure over our joint_log_prob.\n",
    "# The closure makes it so the HMC doesn't try to change the `occurrences` but\n",
    "# instead determines the distributions of other parameters that might generate\n",
    "# the `occurrences` we observed.\n",
    "unnormalized_posterior_log_prob = lambda *args: joint_log_prob(occurrences, *args)\n",
    "\n",
    "# Initialize the step_size. (It will be automatically adapted.)\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "    step_size = tf.get_variable(\n",
    "        name='step_size',\n",
    "        initializer=tf.constant(0.5, dtype=tf.float32),\n",
    "        trainable=False,\n",
    "        use_resource=True\n",
    "    )\n",
    "\n",
    "# Defining the HMC\n",
    "hmc = tfp.mcmc.TransformedTransitionKernel(\n",
    "    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=unnormalized_posterior_log_prob,\n",
    "        num_leapfrog_steps=leapfrog_steps,\n",
    "        step_size=step_size,\n",
    "        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(),\n",
    "        state_gradients_are_stopped=True),\n",
    "    bijector=unconstraining_bijectors)\n",
    "\n",
    "# Sampling from the chain.\n",
    "[\n",
    "    posterior_prob_A\n",
    "], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=number_of_steps,\n",
    "    num_burnin_steps=burnin,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=hmc)\n",
    "\n",
    "# Initialize any created variables.\n",
    "init_g = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUVnbqhDVfAx"
   },
   "source": [
    "#### Execute the TF graph to sample from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.637Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q3By4GWdEtQN",
    "outputId": "b40ec87c-cb5e-49c1-e6ff-44ecd1dc7cf3"
   },
   "outputs": [],
   "source": [
    "evaluate(init_g)\n",
    "evaluate(init_l)\n",
    "[\n",
    "    posterior_prob_A_,\n",
    "    kernel_results_,\n",
    "] = evaluate([\n",
    "    posterior_prob_A,\n",
    "    kernel_results,\n",
    "])\n",
    "\n",
    "    \n",
    "print(\"acceptance rate: {}\".format(\n",
    "    kernel_results_.inner_results.is_accepted.mean()))\n",
    "\n",
    "burned_prob_A_trace_ = posterior_prob_A_[burnin:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQUWTY7-LgGv"
   },
   "source": [
    "We plot the posterior distribution of the unknown $p_A$ below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.647Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "w_P52-CRFJPs",
    "outputId": "bbf9109c-d7a8-47e4-814c-bf27aa79afe0"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize(12.5, 4))\n",
    "plt.title(\"Posterior distribution of $p_A$, the true effectiveness of site A\")\n",
    "plt.vlines(prob_true, 0, 90, linestyle=\"--\", label=\"true $p_A$ (unknown)\")\n",
    "plt.hist(burned_prob_A_trace_, bins=25, histtype=\"stepfilled\", normed=True)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LdLJ2iriIAyI"
   },
   "source": [
    "Our posterior distribution puts most weight near the true value of $p_A$, but also some weights in the tails. This is a measure of how uncertain we should be, given our observations. Try changing the number of observations, `N`, and observe how the posterior distribution changes.\n",
    "\n",
    "### *A* and *B* Together\n",
    "\n",
    "A similar analysis can be done for site B's response data to determine the analogous $p_B$. But what we are really interested in is the *difference* between $p_A$ and $p_B$. Let's infer $p_A$, $p_B$, *and* $\\text{delta} = p_A - p_B$, all at once. We can do this using TFP's deterministic variables. (We'll assume for this exercise that $p_B = 0.04$, so $\\text{delta} = 0.01$, $N_B = 750$ (significantly less than $N_A$) and we will simulate site B's data like we did for site A's data ). Our model now looks like the following:\n",
    "\n",
    "$$\\begin{align*}\n",
    "p_A &\\sim \\text{Uniform}[\\text{low}=0,\\text{high}=1) \\\\\n",
    "p_B &\\sim \\text{Uniform}[\\text{low}=0,\\text{high}=1) \\\\\n",
    "X\\ &\\sim \\text{Bernoulli}(\\text{prob}=p) \\\\\n",
    "\\text{for }  i &= 1\\ldots N: \\\\\n",
    " X_i\\ &\\sim \\text{Bernoulli}(p_i)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.657Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "yPDLHl6RIAyJ",
    "outputId": "f04783fe-3111-41b5-cfb9-7166b6f4827d"
   },
   "outputs": [],
   "source": [
    "reset_sess()\n",
    "\n",
    "#these two quantities are unknown to us.\n",
    "true_prob_A_ = 0.05\n",
    "true_prob_B_ = 0.04\n",
    "\n",
    "#notice the unequal sample sizes -- no problem in Bayesian analysis.\n",
    "N_A_ = 1500\n",
    "N_B_ = 750\n",
    "\n",
    "#generate some observations\n",
    "observations_A = tfd.Bernoulli(name=\"obs_A\", \n",
    "                          probs=true_prob_A_).sample(sample_shape=N_A_, seed=6.45)\n",
    "observations_B = tfd.Bernoulli(name=\"obs_B\", \n",
    "                          probs=true_prob_B_).sample(sample_shape=N_B_, seed=6.45)\n",
    "[ \n",
    "    observations_A_,\n",
    "    observations_B_,\n",
    "] = evaluate([ \n",
    "    observations_A, \n",
    "    observations_B, \n",
    "])\n",
    "\n",
    "print(\"Obs from Site A: \", observations_A_[:30], \"...\")\n",
    "print(\"Observed Prob_A: \", np.mean(observations_A_), \"...\")\n",
    "print(\"Obs from Site B: \", observations_B_[:30], \"...\")\n",
    "print(\"Observed Prob_B: \", np.mean(observations_B_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LDzYsDVgMgsz"
   },
   "source": [
    "Below we run inference over the new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.670Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "7ghHBEdXYtxV"
   },
   "outputs": [],
   "source": [
    "def delta(prob_A, prob_B):\n",
    "    \"\"\"\n",
    "    Defining the deterministic delta function. This is our unknown of interest.\n",
    "        \n",
    "    Args:\n",
    "      prob_A: scalar estimate of the probability of a 1 appearing in \n",
    "                observation set A\n",
    "      prob_B: scalar estimate of the probability of a 1 appearing in \n",
    "                observation set B\n",
    "    Returns: \n",
    "      Difference between prob_A and prob_B\n",
    "    \"\"\"\n",
    "    return prob_A - prob_B\n",
    "\n",
    "  \n",
    "def double_joint_log_prob(observations_A_, observations_B_, \n",
    "                   prob_A, prob_B):\n",
    "    \"\"\"\n",
    "    Joint log probability optimization function.\n",
    "        \n",
    "    Args:\n",
    "      observations_A: An array of binary values representing the set of \n",
    "                      observations for site A\n",
    "      observations_B: An array of binary values representing the set of \n",
    "                      observations for site B \n",
    "      prob_A: scalar estimate of the probability of a 1 appearing in \n",
    "                observation set A\n",
    "      prob_B: scalar estimate of the probability of a 1 appearing in \n",
    "                observation set B \n",
    "    Returns: \n",
    "      Joint log probability optimization function.\n",
    "    \"\"\"\n",
    "    tfd = tfp.distributions\n",
    "  \n",
    "    rv_prob_A = tfd.Uniform(low=0., high=1.)\n",
    "    rv_prob_B = tfd.Uniform(low=0., high=1.)\n",
    "  \n",
    "    rv_obs_A = tfd.Bernoulli(probs=prob_A)\n",
    "    rv_obs_B = tfd.Bernoulli(probs=prob_B)\n",
    "  \n",
    "    return (\n",
    "        rv_prob_A.log_prob(prob_A)\n",
    "        + rv_prob_B.log_prob(prob_B)\n",
    "        + tf.reduce_sum(rv_obs_A.log_prob(observations_A_))\n",
    "        + tf.reduce_sum(rv_obs_B.log_prob(observations_B_))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.680Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "h0TDeF3IIAyQ"
   },
   "outputs": [],
   "source": [
    "number_of_steps = 37200 #@param {type:\"slider\", min:2000, max:50000, step:100}\n",
    "#@markdown (Default is 18000).\n",
    "burnin = 1000 #@param {type:\"slider\", min:0, max:30000, step:100}\n",
    "#@markdown (Default is 1000).\n",
    "leapfrog_steps=3 #@param {type:\"slider\", min:1, max:9, step:1}\n",
    "#@markdown (Default is 6).\n",
    "\n",
    "\n",
    "# Set the chain's start state.\n",
    "initial_chain_state = [    \n",
    "    tf.reduce_mean(tf.to_float(observations_A)) * tf.ones([], dtype=tf.float32, name=\"init_prob_A\"),\n",
    "    tf.reduce_mean(tf.to_float(observations_B)) * tf.ones([], dtype=tf.float32, name=\"init_prob_B\")\n",
    "]\n",
    "\n",
    "# Since HMC operates over unconstrained space, we need to transform the\n",
    "# samples so they live in real-space.\n",
    "unconstraining_bijectors = [\n",
    "    tfp.bijectors.Identity(),   # Maps R to R.\n",
    "    tfp.bijectors.Identity()    # Maps R to R.\n",
    "]\n",
    "\n",
    "# Define a closure over our joint_log_prob.\n",
    "unnormalized_posterior_log_prob = lambda *args: double_joint_log_prob(observations_A_, observations_B_, *args)\n",
    "\n",
    "# Initialize the step_size. (It will be automatically adapted.)\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "    step_size = tf.get_variable(\n",
    "        name='step_size',\n",
    "        initializer=tf.constant(0.5, dtype=tf.float32),\n",
    "        trainable=False,\n",
    "        use_resource=True\n",
    "    )\n",
    "\n",
    "# Defining the HMC\n",
    "hmc=tfp.mcmc.TransformedTransitionKernel(\n",
    "    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=unnormalized_posterior_log_prob,\n",
    "        num_leapfrog_steps=3,\n",
    "        step_size=step_size,\n",
    "        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(),\n",
    "        state_gradients_are_stopped=True),\n",
    "    bijector=unconstraining_bijectors)\n",
    "\n",
    "# Sample from the chain.\n",
    "[\n",
    "    posterior_prob_A,\n",
    "    posterior_prob_B\n",
    "], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=number_of_steps,\n",
    "    num_burnin_steps=burnin,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=hmc)\n",
    "\n",
    "# Initialize any created variables.\n",
    "init_g = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "beUUmGMbdrRr"
   },
   "source": [
    "#### Execute the TF graph to sample from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.698Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HTYITb9fdqIe",
    "outputId": "8bcb70ee-4c1c-4347-b0b8-6bdb6e6d2aa6"
   },
   "outputs": [],
   "source": [
    "evaluate(init_g)\n",
    "evaluate(init_l)\n",
    "[\n",
    "    posterior_prob_A_,\n",
    "    posterior_prob_B_,\n",
    "    kernel_results_\n",
    "] = evaluate([\n",
    "    posterior_prob_A,\n",
    "    posterior_prob_B,\n",
    "    kernel_results\n",
    "])\n",
    "    \n",
    "print(\"acceptance rate: {}\".format(\n",
    "    kernel_results_.inner_results.is_accepted.mean()))\n",
    "\n",
    "burned_prob_A_trace_ = posterior_prob_A_[burnin:]\n",
    "burned_prob_B_trace_ = posterior_prob_B_[burnin:]\n",
    "burned_delta_trace_ = (posterior_prob_A_ - posterior_prob_B_)[burnin:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YaD67cOkIAyT"
   },
   "source": [
    "Below we plot the posterior distributions for the three unknowns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.709Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 742
    },
    "colab_type": "code",
    "id": "PpBXqVKELHRO",
    "outputId": "2296d675-b1c4-409b-e314-c5e4d8b53321"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize(12.5, 12.5))\n",
    "\n",
    "#histogram of posteriors\n",
    "\n",
    "ax = plt.subplot(311)\n",
    "\n",
    "plt.xlim(0, .1)\n",
    "plt.hist(burned_prob_A_trace_, histtype='stepfilled', bins=25, alpha=0.85,\n",
    "         label=\"posterior of $p_A$\", color=TFColor[0], density=True)\n",
    "plt.vlines(true_prob_A_, 0, 80, linestyle=\"--\", label=\"true $p_A$ (unknown)\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Posterior distributions of $p_A$, $p_B$, and delta unknowns\")\n",
    "\n",
    "ax = plt.subplot(312)\n",
    "\n",
    "plt.xlim(0, .1)\n",
    "plt.hist(burned_prob_B_trace_, histtype='stepfilled', bins=25, alpha=0.85,\n",
    "         label=\"posterior of $p_B$\", color=TFColor[2], density=True)\n",
    "plt.vlines(true_prob_B_, 0, 80, linestyle=\"--\", label=\"true $p_B$ (unknown)\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "ax = plt.subplot(313)\n",
    "plt.hist(burned_delta_trace_, histtype='stepfilled', bins=30, alpha=0.85,\n",
    "         label=\"posterior of delta\", color=TFColor[6], density=True)\n",
    "plt.vlines(true_prob_A_ - true_prob_B_, 0, 60, linestyle=\"--\",\n",
    "           label=\"true delta (unknown)\")\n",
    "plt.vlines(0, 0, 60, color=\"black\", alpha=0.2)\n",
    "plt.legend(loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hn-G0eFwIAyh"
   },
   "source": [
    "Notice that as a result of `N_B < N_A`, i.e. we have less data from site B, our posterior distribution of $p_B$ is fatter, implying we are less certain about the true value of $p_B$ than we are of $p_A$.  \n",
    "\n",
    "With respect to the posterior distribution of $\\text{delta}$, we can see that the majority of the distribution is above $\\text{delta}=0$, implying there site A's response is likely better than site B's response. The probability this inference is incorrect is easily computable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.719Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "nZxDurxyIAyh",
    "outputId": "7cb41824-d97a-4b53-e12b-169d009251e7"
   },
   "outputs": [],
   "source": [
    "# Count the number of samples less than 0, i.e. the area under the curve\n",
    "# before 0, represent the probability that site A is worse than site B.\n",
    "print(\"Probability site A is WORSE than site B: %.3f\" % \\\n",
    "    np.mean(burned_delta_trace_ < 0))\n",
    "\n",
    "print(\"Probability site A is BETTER than site B: %.3f\" % \\\n",
    "    np.mean(burned_delta_trace_ > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q8cAEzbUIAyl"
   },
   "source": [
    "If this probability is too high for comfortable decision-making, we can perform more trials on site B (as site B has less samples to begin with, each additional data point for site B contributes more inferential \"power\" than each additional data point for site A). \n",
    "\n",
    "Try playing with the parameters `true_prob_A`, `true_prob_B`, `N_A`, and `N_B`, to see what the posterior of $\\text{delta}$ looks like. Notice in all this, the difference in sample sizes between site A and site B was never mentioned: it naturally fits into Bayesian analysis.\n",
    "\n",
    "I hope the readers feel this style of A/B testing is more natural than hypothesis testing, which has probably confused more than helped practitioners. Later in this book, we will see two extensions of this model: the first to help dynamically adjust for bad sites, and the second will improve the speed of this computation by reducing the analysis to a single equation.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f-jxOi70IAyl"
   },
   "source": [
    "## An algorithm for human deceit\n",
    "\n",
    "Social data has an additional layer of interest as people are not always honest with responses, which adds a further complication into inference. For example, simply asking individuals \"Have you ever cheated on a test?\" will surely contain some rate of dishonesty. What you can say for certain is that the true rate is less than your observed rate (assuming individuals lie *only* about *not cheating*; I cannot imagine one who would admit \"Yes\" to cheating when in fact they hadn't cheated). \n",
    "\n",
    "To present an elegant solution to circumventing this dishonesty problem, and to demonstrate Bayesian modeling, we first need to introduce the binomial distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qzCqZqzBDpMa"
   },
   "source": [
    "## The Binomial Distribution\n",
    "\n",
    "The binomial distribution is one of the most popular distributions, mostly because of its simplicity and usefulness. Unlike the other distributions we have encountered thus far in the book, the binomial distribution has 2 parameters: $N$, a positive integer representing $N$ trials or number of instances of potential events, and $p$, the probability of an event occurring in a single trial. Like the Poisson distribution, it is a discrete distribution, but unlike the Poisson distribution, it only weighs integers from $0$ to $N$. The mass distribution looks like:\n",
    "\n",
    "$$P( X = k ) =  {{N}\\choose{k}}  p^k(1-p)^{N-k}$$\n",
    "\n",
    "If $X$ is a binomial random variable with parameters $p$ and $N$, denoted $X \\sim \\text{Bin}(N,p)$, then $X$ is the number of events that occurred in the $N$ trials (obviously $0 \\le X \\le N$). The larger $p$ is (while still remaining between 0 and 1), the more events are likely to occur. The expected value of a binomial is equal to $Np$. Below we plot the mass probability distribution for varying parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.732Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "9I53Ta3maWgJ",
    "outputId": "e097f79a-2032-4e1a-949a-2b26a1004a78"
   },
   "outputs": [],
   "source": [
    "k_values = tf.range(start=0, limit=(N + 1), dtype=tf.float32)\n",
    "random_var_probs_1 = tfd.Binomial(total_count=10., probs=.4).prob(k_values)\n",
    "random_var_probs_2 = tfd.Binomial(total_count=10., probs=.9).prob(k_values)\n",
    "\n",
    "# Execute graph\n",
    "[\n",
    "    k_values_,\n",
    "    random_var_probs_1_,\n",
    "    random_var_probs_2_,\n",
    "] = evaluate([\n",
    "    k_values,\n",
    "    random_var_probs_1,\n",
    "    random_var_probs_2,\n",
    "])\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(12.5, 4))\n",
    "colors = [TFColor[3], TFColor[0]] \n",
    "\n",
    "plt.bar(k_values_ - 0.5, random_var_probs_1_, color=colors[0],\n",
    "        edgecolor=colors[0],\n",
    "        alpha=0.6,\n",
    "        label=\"$N$: %d, $p$: %.1f\" % (10., .4),\n",
    "        linewidth=3)\n",
    "plt.bar(k_values_ - 0.5, random_var_probs_2_, color=colors[1],\n",
    "        edgecolor=colors[1],\n",
    "        alpha=0.6,\n",
    "        label=\"$N$: %d, $p$: %.1f\" % (10., .9),\n",
    "        linewidth=3)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlim(0, 10.5)\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.ylabel(\"$P(X = k)$\")\n",
    "plt.title(\"Probability mass distributions of binomial random variables\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xT-q-ZI0IAys"
   },
   "source": [
    "The special case when $N = 1$ corresponds to the Bernoulli distribution. There is another connection between Bernoulli and Binomial random variables. If we have $X_1, X_2, ... , X_N$ Bernoulli random variables with the same $p$, then $Z = X_1 + X_2 + ... + X_N \\sim \\text{Binomial}(N, p )$.\n",
    "\n",
    "The expected value of a Bernoulli random variable is $p$. This can be seen by noting the more general Binomial random variable has expected value $Np$ and setting $N=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9HmW-50PIAyv"
   },
   "source": [
    "## Example: Cheating among students\n",
    "\n",
    "We will use the binomial distribution to determine the frequency of students cheating during an exam. If we let $N$ be the total number of students who took the exam, and assuming each student is interviewed post-exam (answering without consequence), we will receive integer $X$ \"Yes I did cheat\" answers. We then find the posterior distribution of $p$, given $N$, some specified prior on $p$, and observed data $X$. \n",
    "\n",
    "This is a completely absurd model. No student, even with a free-pass against punishment, would admit to cheating. What we need is a better *algorithm* to ask students if they had cheated. Ideally the algorithm should encourage individuals to be honest while preserving privacy. The following proposed algorithm is a solution I greatly admire for its ingenuity and effectiveness:\n",
    "\n",
    "> In the interview process for each student, the student flips a coin, hidden from the interviewer. The student agrees to answer honestly if the coin comes up heads. Otherwise, if the coin comes up tails, the student (secretly) flips the coin again, and answers \"Yes, I did cheat\" if the coin flip lands heads, and \"No, I did not cheat\", if the coin flip lands tails. This way, the interviewer does not know if a \"Yes\" was the result of a guilty plea, or a Heads on a second coin toss. Thus privacy is preserved and the researchers receive honest answers. \n",
    "\n",
    "I call this the Privacy Algorithm. One could of course argue that the interviewers are still receiving false data since some *Yes*'s are not confessions but instead randomness, but an alternative perspective is that the researchers are discarding approximately half of their original dataset since half of the responses will be noise. But they have gained a systematic data generation process that can be modeled. Furthermore, they do not have to incorporate (perhaps somewhat naively) the possibility of deceitful answers. We can use TFP to dig through this noisy model, and find a posterior distribution for the true frequency of liars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gPcqaqxMIAyw"
   },
   "source": [
    "Suppose 100 students are being surveyed for cheating, and we wish to find $p$, the proportion of cheaters. There are a few ways we can model this in TFP. I'll demonstrate the most explicit way, and later show a simplified version. Both versions arrive at the same inference. In our data-generation model, we sample $p$, the true proportion of cheaters, from a prior. Since we are quite ignorant about $p$, we will assign it a $\\text{Uniform}(0,1)$ prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.746Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "LIg-xs2LIAyw"
   },
   "outputs": [],
   "source": [
    "reset_sess()\n",
    "\n",
    "N = 100\n",
    "p = tfd.Uniform(name=\"freq_cheating\", low=0., high=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7L0nMGmrIAy0"
   },
   "source": [
    "Again, thinking of our data-generation model, we assign Bernoulli random variables to the 100 students: 1 implies they cheated and 0 implies they did not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.756Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "aXxhrJdtIAy0",
    "outputId": "57d6feb0-f99d-460e-ea0f-c6ea67392c16"
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "reset_sess()\n",
    "p = tfd.Uniform(name=\"freq_cheating\", low=0., high=1.)\n",
    "true_answers = tfd.Bernoulli(name=\"truths\", \n",
    "                             probs=p.sample()).sample(sample_shape=N, \n",
    "                                                      seed=5)\n",
    "# Execute graph\n",
    "[\n",
    "    true_answers_,\n",
    "] = evaluate([\n",
    "    true_answers,\n",
    "])\n",
    "\n",
    "print(true_answers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNB9WGYcIAy4"
   },
   "source": [
    "If we carry out the algorithm, the next step that occurs is the first coin-flip each student makes. This can be modeled again by sampling 100 Bernoulli random variables with $p=1/2$: denote a 1 as a *Heads* and 0 a *Tails*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.766Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "68t8O39EIAy4",
    "outputId": "8c2114dd-6c5b-4db6-a10d-23c73c0ede87"
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "first_coin_flips = tfd.Bernoulli(name=\"first_flips\", \n",
    "                                 probs=0.5).sample(sample_shape=N, \n",
    "                                                   seed=5)\n",
    "# Execute graph\n",
    "[\n",
    "    first_coin_flips_,\n",
    "] = evaluate([\n",
    "    first_coin_flips,\n",
    "])\n",
    "\n",
    "print(first_coin_flips_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-ZnScpWIAzA"
   },
   "source": [
    "Although *not everyone* flips a second time, we can still model the possible realization of second coin-flips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.775Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "acP-4TAfIAzB",
    "outputId": "f4c3e132-3dee-4eb9-afd7-6c22fd128b37"
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "second_coin_flips = tfd.Bernoulli(name=\"second_flips\", \n",
    "                                  probs=0.5).sample(sample_shape=N, \n",
    "                                                    seed=5)\n",
    "# Execute graph\n",
    "[\n",
    "    second_coin_flips_,\n",
    "] = evaluate([\n",
    "    second_coin_flips,\n",
    "])\n",
    "\n",
    "print(second_coin_flips_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uiVbAjoTIAzI"
   },
   "source": [
    "Using these variables, we can return a possible realization of the *observed proportion* of \"Yes\" responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.785Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BJxN0jmBIAzJ"
   },
   "outputs": [],
   "source": [
    "def observed_proportion_calc(t_a = true_answers, \n",
    "                             fc = first_coin_flips,\n",
    "                             sc = second_coin_flips):\n",
    "    \"\"\"\n",
    "    Unnormalized log posterior distribution function\n",
    "        \n",
    "    Args:\n",
    "      t_a: array of binary variables representing the true answers\n",
    "      fc: array of binary variables representing the simulated first flips \n",
    "      sc: array of binary variables representing the simulated second flips\n",
    "    Returns: \n",
    "      Observed proportion of coin flips\n",
    "    Closure over: N\n",
    "    \"\"\"\n",
    "    observed = fc * t_a + (1 - fc) * sc\n",
    "    observed_proportion = tf.to_float(tf.reduce_sum(observed)) / tf.to_float(N)\n",
    "    \n",
    "    return tf.to_float(observed_proportion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OoIWHbsNIAzL"
   },
   "source": [
    "The line `fc*t_a + (1-fc)*sc` contains the heart of the Privacy algorithm. Elements in this array are 1 *if and only if* i) the first toss is heads and the student cheated or ii) the first toss is tails, and the second is heads, and are 0 else. Finally, the last line sums this vector and divides by `float(N)`, producing a proportion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.794Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ma5VwRSNIAzM",
    "outputId": "92449808-a8fe-4bd4-973b-73e22bb27eb5"
   },
   "outputs": [],
   "source": [
    "observed_proportion_val = observed_proportion_calc(t_a=true_answers_,\n",
    "                                                   fc=first_coin_flips_,\n",
    "                                                   sc=second_coin_flips_)\n",
    "# Execute graph\n",
    "[\n",
    "    observed_proportion_val_,\n",
    "] = evaluate([\n",
    "    observed_proportion_val,\n",
    "])\n",
    "\n",
    "print(observed_proportion_val_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HNoBM39rIAzQ"
   },
   "source": [
    "Next we need a dataset. After performing our coin-flipped interviews the researchers received 35 \"Yes\" responses. To put this into a relative perspective, if there truly were no cheaters, we should expect to see on average 1/4 of all responses being a \"Yes\" (half chance of having first coin land Tails, and another half chance of having second coin land Heads), so about 25 responses in a cheat-free world. On the other hand, if *all students cheated*, we should expected to see approximately 3/4 of all responses be \"Yes\". \n",
    "\n",
    "The researchers observe a Binomial random variable, with `N = 100` and `total_yes = 35`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.804Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SLcH6ZPsIAzR"
   },
   "outputs": [],
   "source": [
    "total_count = 100\n",
    "total_yes = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.812Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-kWZd1ygofav"
   },
   "outputs": [],
   "source": [
    "def coin_joint_log_prob(total_yes, total_count, lies_prob):\n",
    "    \"\"\"\n",
    "    Joint log probability optimization function.\n",
    "      \n",
    "    Args:\n",
    "      headsflips: Integer for total number of observed heads flips\n",
    "      N: Integer for number of total observation\n",
    "      lies_prob: Test probability of a heads flip (1) for a Binomial distribution\n",
    "    Returns: \n",
    "      Joint log probability optimization function.\n",
    "    \"\"\"\n",
    "  \n",
    "    rv_lies_prob = tfd.Uniform(name=\"rv_lies_prob\",low=0., high=1.)\n",
    "\n",
    "    cheated = tfd.Bernoulli(probs=tf.to_float(lies_prob)).sample(total_count)\n",
    "    first_flips = tfd.Bernoulli(probs=0.5).sample(total_count)\n",
    "    second_flips = tfd.Bernoulli(probs=0.5).sample(total_count)\n",
    "    observed_probability = tf.reduce_sum(tf.to_float(\n",
    "        cheated * first_flips + (1 - first_flips) * second_flips)) / total_count\n",
    "\n",
    "    rv_yeses = tfd.Binomial(name=\"rv_yeses\",\n",
    "                total_count=float(total_count),\n",
    "                probs=observed_probability)\n",
    "    \n",
    "    return (\n",
    "        rv_lies_prob.log_prob(lies_prob)\n",
    "        + tf.reduce_sum(rv_yeses.log_prob(tf.to_float(total_yes)))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZC4TITlIAzV"
   },
   "source": [
    "Below we add all the variables of interest to our Metropolis-Hastings sampler and run our black-box algorithm over the model. It's important to note that we're using a Metropolis-Hastings MCMC instead of a Hamiltonian since we're sampling inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.822Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Awl3GmgjIAzV"
   },
   "outputs": [],
   "source": [
    "burnin = 15000\n",
    "num_of_steps = 40000\n",
    "total_count=100\n",
    "\n",
    "# Set the chain's start state.\n",
    "initial_chain_state = [\n",
    "    0.4 * tf.ones([], dtype=tf.float32, name=\"init_prob\")\n",
    "]\n",
    "\n",
    "# Define a closure over our joint_log_prob.\n",
    "unnormalized_posterior_log_prob = lambda *args: coin_joint_log_prob(total_yes, total_count,  *args)\n",
    "\n",
    "# Defining the Metropolis-Hastings\n",
    "# We use a Metropolis-Hastings method here instead of Hamiltonian method\n",
    "# because the coin flips in the above example are non-differentiable and cannot\n",
    "# bue used with HMC.\n",
    "metropolis=tfp.mcmc.RandomWalkMetropolis(\n",
    "    target_log_prob_fn=unnormalized_posterior_log_prob,\n",
    "    seed=54)\n",
    "\n",
    "# Sample from the chain.\n",
    "[\n",
    "    posterior_p\n",
    "], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=num_of_steps,\n",
    "    num_burnin_steps=burnin,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=metropolis,\n",
    "    parallel_iterations=1,\n",
    "    name='Metropolis-Hastings_coin-flips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lq0OtJDCufOu"
   },
   "source": [
    "##### Executing the TF graph to sample from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.831Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x--bCsBrr91E",
    "outputId": "cfab9154-992d-418d-ba2e-d16f1cae1e65"
   },
   "outputs": [],
   "source": [
    "# Content Warning: This cell can take up to 5 minutes in Graph Mode\n",
    "[\n",
    "    posterior_p_,\n",
    "    kernel_results_\n",
    "] = evaluate([\n",
    "    posterior_p,\n",
    "    kernel_results,\n",
    "])\n",
    " \n",
    "print(\"acceptance rate: {}\".format(\n",
    "    kernel_results_.is_accepted.mean()))\n",
    "# print(\"prob_p trace: \", posterior_p_)\n",
    "# print(\"prob_p burned trace: \", posterior_p_[burnin:])\n",
    "burned_cheating_freq_samples_ = posterior_p_[burnin:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QhCgk98ynq5s"
   },
   "source": [
    "And finally we can plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.840Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "JoKNmLpxB1yt",
    "outputId": "b793d81e-288a-4d80-8a37-f34f19e50aab"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize(12.5, 6))\n",
    "p_trace_ = burned_cheating_freq_samples_\n",
    "plt.hist(p_trace_, histtype=\"stepfilled\", normed=True, alpha=0.85, bins=30, \n",
    "         label=\"posterior distribution\", color=TFColor[3])\n",
    "plt.vlines([.1, .40], [0, 0], [5, 5], alpha=0.3)\n",
    "plt.xlim(0, 1)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqDMt8xyIAzd"
   },
   "source": [
    "With regards to the above plot, we are still pretty uncertain about what the true frequency of cheaters might be, but we have narrowed it down to a range between 0.1 to 0.4 (marked by the solid lines). This is pretty good, as *a priori* we had no idea how many students might have cheated (hence the uniform distribution for our prior). On the other hand, it is also pretty bad since there is a .3 length window the true value most likely lives in. Have we even gained anything, or are we still too uncertain about the true frequency? \n",
    "\n",
    "I would argue, yes, we have discovered something. It is implausible, according to our posterior, that there are *no cheaters*, i.e. the posterior assigns low probability to $p=0$. Since we started with an uniform prior, treating all values of $p$ as equally plausible, but the data ruled out $p=0$ as a possibility, we can be confident that there were cheaters. \n",
    "\n",
    "This kind of algorithm can be used to gather private information from users and be *reasonably* confident that the data, though noisy, is truthful. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y0bK5tMAIAze"
   },
   "source": [
    "### Alternative TFP Model\n",
    "\n",
    "Given a value for $p$ (which from our god-like position we know), we can find the probability the student will answer yes: \n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\text{\"Yes\"}) &= P( \\text{Heads on first coin} )P( \\text{cheater} ) + P( \\text{Tails on first coin} )P( \\text{Heads on second coin} ) \\\\\n",
    "&= \\frac{1}{2}p + \\frac{1}{2}\\frac{1}{2}\\\\\n",
    "&= \\frac{p}{2} + \\frac{1}{4}\n",
    "\\end{align}\n",
    "$$\n",
    "Thus, knowing $p$ we know the probability a student will respond \"Yes\". In TFP, we can create a deterministic function to evaluate the probability of responding \"Yes\", given $p$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.851Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "wBps82qhIAzf"
   },
   "outputs": [],
   "source": [
    "reset_sess()\n",
    "\n",
    "p_new = tfd.Uniform(name=\"new_freq_cheating\", \n",
    "                low=0., \n",
    "                high=1.)\n",
    "p_new_skewed = tfd.Deterministic(name=\"p_skewed\", \n",
    "                             loc=(0.5 * p_new.sample(seed=0.5) + 0.25)).sample(seed=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m_kgk64TIAzh"
   },
   "source": [
    "I could have typed `p_skewed = 0.5 * p + 0.25` instead for a one-liner, as the elementary operations of addition and scalar multiplication will implicitly create a deterministic variable, but I wanted to make the determinism explicit for clarity's sake. \n",
    "\n",
    "If we know the probability of respondents saying \"Yes\", which is `p_skewed`, and we have $N=100$ students, the number of \"Yes\" responses is a binomial random variable with parameters `N` and `p_skewed`.\n",
    "\n",
    "This is where we include our observed 35 \"Yes\" responses out of a total of 100 which are passed to the `joint_log_prob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.861Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GJ2jFKI7ofa9"
   },
   "outputs": [],
   "source": [
    "N = 100.\n",
    "total_yes = 35.\n",
    "\n",
    "def alt_joint_log_prob(yes_responses, N, prob_cheating):\n",
    "    \"\"\"\n",
    "    Alternative joint log probability optimization function.\n",
    "        \n",
    "    Args:\n",
    "      yes_responses: Integer for total number of affirmative responses\n",
    "      N: Integer for number of total observation\n",
    "      prob_cheating: Test probability of a student actually cheating\n",
    "    Returns: \n",
    "      Joint log probability optimization function.\n",
    "    \"\"\"\n",
    "    tfd = tfp.distributions\n",
    "  \n",
    "    rv_prob = tfd.Uniform(name=\"rv_new_freq_cheating\", low=0., high=1.)\n",
    "    prob_skewed = 0.5 * prob_cheating + 0.25\n",
    "    rv_yes_responses = tfd.Binomial(name=\"rv_yes_responses\",\n",
    "                                     total_count=tf.to_float(N), \n",
    "                                     probs=prob_skewed)\n",
    "\n",
    "    return (\n",
    "        rv_prob.log_prob(prob_cheating)\n",
    "        + tf.reduce_sum(rv_yes_responses.log_prob(tf.to_float(yes_responses)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0clIAcyHIAzj"
   },
   "source": [
    "\n",
    "Below we add all the variables of interest to our HMC component-defining cell and run our black-box algorithm over the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.872Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "C5QLZ17e5u6t"
   },
   "outputs": [],
   "source": [
    "number_of_steps = 25000\n",
    "burnin = 2500\n",
    "\n",
    "# Set the chain's start state.\n",
    "initial_chain_state = [\n",
    "    0.2 * tf.ones([], dtype=tf.float32, name=\"init_skewed_p\")\n",
    "]\n",
    "\n",
    "# Since HMC operates over unconstrained space, we need to transform the\n",
    "# samples so they live in real-space.\n",
    "unconstraining_bijectors = [\n",
    "    tfp.bijectors.Sigmoid(),   # Maps [0,1] to R.\n",
    "]\n",
    "\n",
    "# Define a closure over our joint_log_prob.\n",
    "# unnormalized_posterior_log_prob = lambda *args: alt_joint_log_prob(headsflips, total_yes, N, *args)\n",
    "unnormalized_posterior_log_prob = lambda *args: alt_joint_log_prob(total_yes, N, *args)\n",
    "\n",
    "# Initialize the step_size. (It will be automatically adapted.)\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "    step_size = tf.get_variable(\n",
    "        name='skewed_step_size',\n",
    "        initializer=tf.constant(0.5, dtype=tf.float32),\n",
    "        trainable=False,\n",
    "        use_resource=True\n",
    "    ) \n",
    "\n",
    "# Defining the HMC\n",
    "hmc=tfp.mcmc.TransformedTransitionKernel(\n",
    "    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=unnormalized_posterior_log_prob,\n",
    "        num_leapfrog_steps=2,\n",
    "        step_size=step_size,\n",
    "        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(),\n",
    "        state_gradients_are_stopped=True),\n",
    "    bijector=unconstraining_bijectors)\n",
    "\n",
    "# Sample from the chain.\n",
    "[\n",
    "    posterior_skewed_p\n",
    "], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=number_of_steps,\n",
    "    num_burnin_steps=burnin,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=hmc)\n",
    "\n",
    "# Initialize any created variables.\n",
    "# This prevents a FailedPreconditionError\n",
    "init_g = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJYLS8EysHqj"
   },
   "source": [
    "#### Execute the TF graph to sample from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.881Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ALvEN1yQkTIx",
    "outputId": "2bedf661-d0d3-45d2-fbba-c1ca8c85bb6b"
   },
   "outputs": [],
   "source": [
    "# This cell may take 5 minutes in Graph Mode\n",
    "evaluate(init_g)\n",
    "evaluate(init_l)\n",
    "[\n",
    "    posterior_skewed_p_,\n",
    "    kernel_results_\n",
    "] = evaluate([\n",
    "    posterior_skewed_p,\n",
    "    kernel_results\n",
    "])\n",
    "\n",
    "    \n",
    "print(\"acceptance rate: {}\".format(\n",
    "    kernel_results_.inner_results.is_accepted.mean()))\n",
    "# print(\"final step size: {}\".format(\n",
    "#     kernel_results_.inner_results.extra.step_size_assign[-100:].mean()))\n",
    "\n",
    "# print(\"p_skewed trace: \", posterior_skewed_p_)\n",
    "# print(\"p_skewed burned trace: \", posterior_skewed_p_[burnin:])\n",
    "freq_cheating_samples_ = posterior_skewed_p_[burnin:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ye0uC_c-xrWf"
   },
   "source": [
    "Now we can plot our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.891Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "_P5Z_uySgi-S",
    "outputId": "0b085693-c710-449f-c25a-730abbde9a92"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize(12.5, 6))\n",
    "p_trace_ = freq_cheating_samples_\n",
    "plt.hist(p_trace_, histtype=\"stepfilled\", normed=True, alpha=0.85, bins=30, \n",
    "         label=\"posterior distribution\", color=TFColor[3])\n",
    "plt.vlines([.1, .40], [0, 0], [5, 5], alpha=0.2)\n",
    "plt.xlim(0, 1)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lxt6fSRvIAzy"
   },
   "source": [
    "The remainder of this chapter examines some practical examples of TFP and TFP modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KMoiodMmIAzy"
   },
   "source": [
    "## Example: Challenger Space Shuttle Disaster <span id=\"challenger\"/>\n",
    "\n",
    "On January 28, 1986, the twenty-fifth flight of the U.S. space shuttle program ended in disaster when one of the rocket boosters of the Shuttle Challenger exploded shortly after lift-off, killing all seven crew members. The presidential commission on the accident concluded that it was caused by the failure of an O-ring in a field joint on the rocket booster, and that this failure was due to a faulty design that made the O-ring unacceptably sensitive to a number of factors including outside temperature. Of the previous 24 flights, data were available on failures of O-rings on 23, (one was lost at sea), and these data were discussed on the evening preceding the Challenger launch, but unfortunately only the data corresponding to the 7 flights on which there was a damage incident were considered important and these were thought to show no obvious trend. The data are shown below (see [1]):\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.902Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tlPZvBWkg5g-",
    "outputId": "79564040-3187-4d95-b5d6-fd1fe4d0f332"
   },
   "outputs": [],
   "source": [
    "reset_sess()\n",
    "\n",
    "import wget\n",
    "url = 'https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter2_MorePyMC/data/challenger_data.csv'\n",
    "filename = wget.download(url)\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.911Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "colab_type": "code",
    "id": "BNOqG_9zIAzz",
    "outputId": "a4602219-454f-4e8c-c3f5-8b905e4931d5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize(12.5, 3.5))\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "challenger_data_ = np.genfromtxt(\"challenger_data.csv\", skip_header=1,\n",
    "                                usecols=[1, 2], missing_values=\"NA\",\n",
    "                                delimiter=\",\")\n",
    "#drop the NA values\n",
    "challenger_data_ = challenger_data_[~np.isnan(challenger_data_[:, 1])]\n",
    "\n",
    "#plot it, as a function of tempature (the first column)\n",
    "print(\"Temp (F), O-Ring failure?\")\n",
    "print(challenger_data_)\n",
    "\n",
    "plt.scatter(challenger_data_[:, 0], challenger_data_[:, 1], s=75, color=\"k\",\n",
    "            alpha=0.5)\n",
    "plt.yticks([0, 1])\n",
    "plt.ylabel(\"Damage Incident?\")\n",
    "plt.xlabel(\"Outside temperature (Fahrenheit)\")\n",
    "plt.title(\"Defects of the Space Shuttle O-Rings vs temperature\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "El9Z_4ulIAz3"
   },
   "source": [
    "It looks clear that *the probability* of damage incidents occurring increases as the outside temperature decreases. We are interested in modeling the probability here because it does not look like there is a strict cutoff point between temperature and a damage incident occurring. The best we can do is ask \"At temperature $t$, what is the probability of a damage incident?\". The goal of this example is to answer that question.\n",
    "\n",
    "We need a function of temperature, call it $p(t)$, that is bounded between 0 and 1 (so as to model a probability) and changes from 1 to 0 as we increase temperature. There are actually many such functions, but the most popular choice is the *logistic function.*\n",
    "\n",
    "$$p(t) = \\frac{1}{ 1 + e^{ \\;\\beta t } } $$\n",
    "\n",
    "In this model, $\\beta$ is the variable we are uncertain about. Below is the function plotted for $\\beta = 1, 3, -5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.919Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "colab_type": "code",
    "id": "U4kW2QIddYEs",
    "outputId": "d8728442-bb57-4029-f98d-b6202ff752d8"
   },
   "outputs": [],
   "source": [
    "def logistic(x, beta):\n",
    "    \"\"\"\n",
    "    Logistic Function\n",
    "        \n",
    "    Args:\n",
    "      x: independent variable\n",
    "      beta: beta term\n",
    "    Returns: \n",
    "      Logistic function\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + tf.exp(beta * x))\n",
    "\n",
    "x_vals = tf.linspace(start=-4., stop=4., num=100)\n",
    "log_beta_1 = logistic(x_vals, 1.)\n",
    "log_beta_3 = logistic(x_vals, 3.)\n",
    "log_beta_m5 = logistic(x_vals, -5.)\n",
    "\n",
    "[\n",
    "    x_vals_,\n",
    "    log_beta_1_,\n",
    "    log_beta_3_,\n",
    "    log_beta_m5_,\n",
    "] = evaluate([\n",
    "    x_vals,\n",
    "    log_beta_1,\n",
    "    log_beta_3,\n",
    "    log_beta_m5,\n",
    "])\n",
    "\n",
    "plt.figure(figsize(12.5, 3))\n",
    "plt.plot(x_vals_, log_beta_1_, label=r\"$\\beta = 1$\", color=TFColor[0])\n",
    "plt.plot(x_vals_, log_beta_3_, label=r\"$\\beta = 3$\", color=TFColor[3])\n",
    "plt.plot(x_vals_, log_beta_m5_, label=r\"$\\beta = -5$\", color=TFColor[6])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H_rdEcFIIAz7"
   },
   "source": [
    "But something is missing. In the plot of the logistic function, the probability changes only near zero, but in our data above the probability changes around 65 to 70. We need to add a *bias* term to our logistic function:\n",
    "\n",
    "$$p(t) = \\frac{1}{ 1 + e^{ \\;\\beta t + \\alpha } } $$\n",
    "\n",
    "Some plots are below, with differing $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.929Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "colab_type": "code",
    "id": "T0iZj_eCIAz8",
    "outputId": "81a83e39-baee-4122-9090-5f21d6db5af4"
   },
   "outputs": [],
   "source": [
    "def logistic(x, beta, alpha=0):\n",
    "    \"\"\"\n",
    "    Logistic Function with offset\n",
    "        \n",
    "    Args:\n",
    "        x: independent variable\n",
    "        beta: beta term \n",
    "        alpha: alpha term\n",
    "    Returns: \n",
    "        Logistic function\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + tf.exp((beta * x) + alpha))\n",
    "\n",
    "x_vals = tf.linspace(start=-4., stop=4., num=100)\n",
    "log_beta_1_alpha_1 = logistic(x_vals, 1, 1)\n",
    "log_beta_3_alpha_m2 = logistic(x_vals, 3, -2)\n",
    "log_beta_m5_alpha_7 = logistic(x_vals, -5, 7)\n",
    "\n",
    "[\n",
    "    x_vals_,\n",
    "    log_beta_1_alpha_1_,\n",
    "    log_beta_3_alpha_m2_,\n",
    "    log_beta_m5_alpha_7_,\n",
    "] = evaluate([\n",
    "    x_vals,\n",
    "    log_beta_1_alpha_1,\n",
    "    log_beta_3_alpha_m2,\n",
    "    log_beta_m5_alpha_7,\n",
    "])\n",
    "\n",
    "plt.figure(figsize(12.5, 3))\n",
    "plt.plot(x_vals_, log_beta_1_, label=r\"$\\beta = 1$\", ls=\"--\", lw=1, color=TFColor[0])\n",
    "plt.plot(x_vals_, log_beta_3_, label=r\"$\\beta = 3$\", ls=\"--\", lw=1, color=TFColor[3])\n",
    "plt.plot(x_vals_, log_beta_m5_, label=r\"$\\beta = -5$\", ls=\"--\", lw=1, color=TFColor[6])\n",
    "plt.plot(x_vals_, log_beta_1_alpha_1_, label=r\"$\\beta = 1, \\alpha = 1$\", color=TFColor[0])\n",
    "plt.plot(x_vals_, log_beta_3_alpha_m2_, label=r\"$\\beta = 3, \\alpha = -2$\", color=TFColor[3])\n",
    "plt.plot(x_vals_, log_beta_m5_alpha_7_, label=r\"$\\beta = -5, \\alpha = 7$\", color=TFColor[6])\n",
    "plt.legend(loc=\"lower left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W_B8n8wuIAz9"
   },
   "source": [
    "Adding a constant term $\\alpha$ amounts to shifting the curve left or right (hence why it is called a *bias*).\n",
    "\n",
    "Let's start modeling this in TFP. The $\\beta, \\alpha$ parameters have no reason to be positive, bounded or relatively large, so they are best modeled by a *Normal random variable*, introduced next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_52Ml-KhIAz9"
   },
   "source": [
    "### Normal distributions\n",
    "\n",
    "A Normal random variable, denoted $X \\sim N(\\mu, 1/\\tau)$, has a distribution with two parameters: the mean, $\\mu$, and the *precision*, $\\tau$. Those familiar with the Normal distribution already have probably seen $\\sigma^2$ instead of $\\tau^{-1}$. They are in fact reciprocals of each other. The change was motivated by simpler mathematical analysis and is an artifact of older Bayesian methods. Just remember: the smaller $\\tau$, the larger the spread of the distribution (i.e. we are more uncertain); the larger $\\tau$, the tighter the distribution (i.e. we are more certain). Regardless, $\\tau$ is always positive. \n",
    "\n",
    "The probability density function of a $N( \\mu, 1/\\tau)$ random variable is:\n",
    "\n",
    "$$ f(x | \\mu, \\tau) = \\sqrt{\\frac{\\tau}{2\\pi}} \\exp\\left( -\\frac{\\tau}{2} (x-\\mu)^2 \\right) $$\n",
    "\n",
    "We plot some different density functions below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.940Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "colab_type": "code",
    "id": "OLw3-8x2hxkm",
    "outputId": "49266251-fb26-412f-b171-f03e09e63bcc"
   },
   "outputs": [],
   "source": [
    "rand_x_vals = tf.linspace(start=-8., stop=7., num=150)\n",
    "\n",
    "density_func_1 = tfd.Normal(loc=float(-2.), scale=float(1./.7)).prob(rand_x_vals)\n",
    "density_func_2 = tfd.Normal(loc=float(0.), scale=float(1./1)).prob(rand_x_vals)\n",
    "density_func_3 = tfd.Normal(loc=float(3.), scale=float(1./2.8)).prob(rand_x_vals)\n",
    "\n",
    "[\n",
    "    rand_x_vals_,\n",
    "    density_func_1_,\n",
    "    density_func_2_,\n",
    "    density_func_3_,\n",
    "] = evaluate([\n",
    "    rand_x_vals,\n",
    "    density_func_1,\n",
    "    density_func_2,\n",
    "    density_func_3,\n",
    "])\n",
    "\n",
    "colors = [TFColor[3], TFColor[0], TFColor[6]]\n",
    "\n",
    "plt.figure(figsize(12.5, 3))\n",
    "plt.plot(rand_x_vals_, density_func_1_,\n",
    "         label=r\"$\\mu = %d, \\tau = %.1f$\" % (-2., .7), color=TFColor[3])\n",
    "plt.fill_between(rand_x_vals_, density_func_1_, color=TFColor[3], alpha=.33)\n",
    "plt.plot(rand_x_vals_, density_func_2_, \n",
    "         label=r\"$\\mu = %d, \\tau = %.1f$\" % (0., 1), color=TFColor[0])\n",
    "plt.fill_between(rand_x_vals_, density_func_2_, color=TFColor[0], alpha=.33)\n",
    "plt.plot(rand_x_vals_, density_func_3_,\n",
    "         label=r\"$\\mu = %d, \\tau = %.1f$\" % (3., 2.8), color=TFColor[6])\n",
    "plt.fill_between(rand_x_vals_, density_func_3_, color=TFColor[6], alpha=.33)\n",
    "\n",
    "plt.legend(loc=r\"upper right\")\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"density function at $x$\")\n",
    "plt.title(r\"Probability distribution of three different Normal random variables\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HAxWLKGcIA0A"
   },
   "source": [
    "A Normal random variable can be take on any real number, but the variable is very likely to be relatively close to $\\mu$. In fact, the expected value of a Normal is equal to its $\\mu$ parameter:\n",
    "\n",
    "$$ E[ X | \\mu, \\tau] = \\mu$$\n",
    "\n",
    "and its variance is equal to the inverse of $\\tau$:\n",
    "\n",
    "$$\\text{Var}( X | \\mu, \\tau ) = \\frac{1}{\\tau}$$\n",
    "\n",
    "\n",
    "\n",
    "Below we continue our modeling of the Challenger space craft:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.951Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "F3DBYxvAIA0B"
   },
   "outputs": [],
   "source": [
    "reset_sess()\n",
    "\n",
    "temperature_ = challenger_data_[:, 0]\n",
    "temperature = tf.convert_to_tensor(temperature_, dtype=tf.float32)\n",
    "D_ = challenger_data_[:, 1]                # defect or not?\n",
    "D = tf.convert_to_tensor(D_, dtype=tf.float32)\n",
    "\n",
    "beta = tfd.Normal(name=\"beta\", loc=0.3, scale=1000.).sample()\n",
    "alpha = tfd.Normal(name=\"alpha\", loc=-15., scale=1000.).sample()\n",
    "p_deterministic = tfd.Deterministic(name=\"p\", loc=1.0/(1. + tf.exp(beta * temperature_ + alpha))).sample()\n",
    "\n",
    "[\n",
    "    prior_alpha_,\n",
    "    prior_beta_,\n",
    "    p_deterministic_,\n",
    "    D_,\n",
    "] = evaluate([\n",
    "    alpha,\n",
    "    beta,\n",
    "    p_deterministic,\n",
    "    D,\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PxOWy25CIA0D"
   },
   "source": [
    "We have our probabilities, but how do we connect them to our observed data? A *Bernoulli* random variable with parameter $p$, denoted $\\text{Ber}(p)$, is a random variable that takes value 1 with probability $p$, and 0 else. Thus, our model can look like:\n",
    "\n",
    "$$ \\text{Defect Incident, }D_i \\sim \\text{Ber}( \\;p(t_i)\\; ), \\;\\; i=1..N$$\n",
    "\n",
    "where $p(t)$ is our logistic function and $t_i$ are the temperatures we have observations about. Notice in the code below we set the values of `beta` and `alpha` to 0 in `initial_chain_state`. The reason for this is that if `beta` and `alpha` are very large, they make `p` equal to 1 or 0. Unfortunately, `tfd.Bernoulli` does not like probabilities of exactly 0 or 1, though they are mathematically well-defined probabilities. So by setting the coefficient values to `0`, we set the variable `p` to be a reasonable starting value. This has no effect on our results, nor does it mean we are including any additional information in our prior. It is simply a computational caveat in TFP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.960Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "vRqoyxqnofbT"
   },
   "outputs": [],
   "source": [
    "def challenger_joint_log_prob(D, temperature_, alpha, beta):\n",
    "    \"\"\"\n",
    "    Joint log probability optimization function.\n",
    "        \n",
    "    Args:\n",
    "      D: The Data from the challenger disaster representing presence or \n",
    "         absence of defect\n",
    "      temperature_: The Data from the challenger disaster, specifically the temperature on \n",
    "         the days of the observation of the presence or absence of a defect\n",
    "      alpha: one of the inputs of the HMC\n",
    "      beta: one of the inputs of the HMC\n",
    "    Returns: \n",
    "      Joint log probability optimization function.\n",
    "    \"\"\"\n",
    "    rv_alpha = tfd.Normal(loc=0., scale=1000.)\n",
    "    rv_beta = tfd.Normal(loc=0., scale=1000.)\n",
    "    logistic_p = 1.0/(1. + tf.exp(beta * tf.to_float(temperature_) + alpha))\n",
    "    rv_observed = tfd.Bernoulli(probs=logistic_p)\n",
    "  \n",
    "    return (\n",
    "        rv_alpha.log_prob(alpha)\n",
    "        + rv_beta.log_prob(beta)\n",
    "        + tf.reduce_sum(rv_observed.log_prob(D))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.968Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "oHU-MbPxs8iL"
   },
   "outputs": [],
   "source": [
    "number_of_steps = 100000 #@param {type:\"slider\", min:25000, max:120000, step:1000}\n",
    "burnin = 95000 #@param {type:\"slider\", min:20000, max:100000, step:1000}\n",
    "\n",
    "# Set the chain's start state.\n",
    "initial_chain_state = [\n",
    "    0. * tf.ones([], dtype=tf.float32, name=\"init_alpha\"),\n",
    "    0. * tf.ones([], dtype=tf.float32, name=\"init_beta\")\n",
    "]\n",
    "\n",
    "# Since HMC operates over unconstrained space, we need to transform the\n",
    "# samples so they live in real-space.\n",
    "unconstraining_bijectors = [\n",
    "    tfp.bijectors.Identity(),\n",
    "    tfp.bijectors.Identity()\n",
    "]\n",
    "\n",
    "# Define a closure over our joint_log_prob.\n",
    "unnormalized_posterior_log_prob = lambda *args: challenger_joint_log_prob(D, temperature_, *args)\n",
    "\n",
    "# Initialize the step_size. (It will be automatically adapted.)\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "    step_size = tf.get_variable(\n",
    "        name='step_size',\n",
    "        initializer=tf.constant(0.5, dtype=tf.float32),\n",
    "        trainable=False,\n",
    "        use_resource=True\n",
    "    )\n",
    "\n",
    "# Defining the HMC\n",
    "hmc=tfp.mcmc.TransformedTransitionKernel(\n",
    "    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=unnormalized_posterior_log_prob,\n",
    "        num_leapfrog_steps=2,\n",
    "        step_size=step_size,\n",
    "        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(),\n",
    "        state_gradients_are_stopped=True),\n",
    "    bijector=unconstraining_bijectors)\n",
    "\n",
    "# Sampling from the chain.\n",
    "[\n",
    "    posterior_alpha,\n",
    "    posterior_beta\n",
    "], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results = number_of_steps,\n",
    "    num_burnin_steps = burnin,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=hmc)\n",
    "\n",
    "# Initialize any created variables for preconditions\n",
    "init_g = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eNkhSXDkthRs"
   },
   "source": [
    "#### Execute the TF graph to sample from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.978Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "XJyZIwoyth2j",
    "outputId": "20d229cd-6fe2-4740-b228-b5a52485d724"
   },
   "outputs": [],
   "source": [
    "# In Graph Mode, this cell can take up to 36 Minutes\n",
    "evaluate(init_g)\n",
    "[\n",
    "    posterior_alpha_,\n",
    "    posterior_beta_,\n",
    "    kernel_results_\n",
    "] = evaluate([\n",
    "    posterior_alpha,\n",
    "    posterior_beta,\n",
    "    kernel_results\n",
    "])\n",
    "    \n",
    "print(\"acceptance rate: {}\".format(\n",
    "    kernel_results_.inner_results.is_accepted.mean()))\n",
    "print(\"final step size: {}\".format(\n",
    "    kernel_results_.inner_results.extra.step_size_assign[-100:].mean()))\n",
    "\n",
    "alpha_samples_ = posterior_alpha_[burnin::8]\n",
    "beta_samples_ = posterior_beta_[burnin::8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xIGyBkilIA0G"
   },
   "source": [
    "We have trained our model on the observed data, now we can sample values from the posterior. Let's look at the posterior distributions for $\\alpha$ and $\\beta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.988Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "Pdgjgw9RiluO",
    "outputId": "ef1c2e5e-b5a9-4cbe-9adb-890e464d8194"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize(12.5, 6))\n",
    "\n",
    "#histogram of the samples:\n",
    "plt.subplot(211)\n",
    "plt.title(r\"Posterior distributions of the variables $\\alpha, \\beta$\")\n",
    "plt.hist(beta_samples_, histtype='stepfilled', bins=35, alpha=0.85,\n",
    "         label=r\"posterior of $\\beta$\", color=TFColor[6], normed=True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.hist(alpha_samples_, histtype='stepfilled', bins=35, alpha=0.85,\n",
    "         label=r\"posterior of $\\alpha$\", color=TFColor[0], normed=True)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gp0QmuZvIA0L"
   },
   "source": [
    "All samples of $\\beta$ are greater than 0. If instead the posterior was centered around 0, we may suspect that $\\beta = 0$, implying that temperature has no effect on the probability of defect. \n",
    "\n",
    "Similarly, all $\\alpha$ posterior values are negative and far away from 0, implying that it is correct to believe that $\\alpha$ is significantly less than 0. \n",
    "\n",
    "Regarding the spread of the data, we are very uncertain about what the true parameters might be (though considering the low sample size and the large overlap of defects-to-nondefects this behaviour is perhaps expected).  \n",
    "\n",
    "Next, let's look at the *expected probability* for a specific value of the temperature. That is, we average over all samples from the posterior to get a likely value for $p(t_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:57.998Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "EIzyJL_3IA0P",
    "outputId": "9d908284-fb3b-42de-a454-aa4dde071694"
   },
   "outputs": [],
   "source": [
    "alpha_samples_1d_ = alpha_samples_[:, None]  # best to make them 1d\n",
    "beta_samples_1d_ = beta_samples_[:, None]\n",
    "\n",
    "\n",
    "beta_mean = tf.reduce_mean(beta_samples_1d_.T[0])\n",
    "alpha_mean = tf.reduce_mean(alpha_samples_1d_.T[0])\n",
    "[ beta_mean_, alpha_mean_ ] = evaluate([ beta_mean, alpha_mean ])\n",
    "\n",
    "\n",
    "print(\"beta mean:\", beta_mean_)\n",
    "print(\"alpha mean:\", alpha_mean_)\n",
    "def logistic(x, beta, alpha=0):\n",
    "    \"\"\"\n",
    "    Logistic function with alpha and beta.\n",
    "        \n",
    "    Args:\n",
    "      x: independent variable\n",
    "      beta: beta term \n",
    "      alpha: alpha term\n",
    "    Returns: \n",
    "      Logistic function\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + tf.exp((beta * x) + alpha))\n",
    "\n",
    "t_ = np.linspace(temperature_.min() - 5, temperature_.max() + 5, 2500)[:, None]\n",
    "p_t = logistic(t_.T, beta_samples_1d_, alpha_samples_1d_)\n",
    "mean_prob_t = logistic(t_.T, beta_mean_, alpha_mean_)\n",
    "[ \n",
    "    p_t_, mean_prob_t_\n",
    "] = evaluate([ \n",
    "    p_t, mean_prob_t\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:58.007Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "Ri4BriJHPJNg",
    "outputId": "8e1e75bb-8ecd-4ad7-e34c-93873e1abbee"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize(12.5, 4))\n",
    "\n",
    "plt.plot(t_, mean_prob_t_.T, lw=3, label=\"average posterior \\nprobability \\\n",
    "of defect\")\n",
    "plt.plot(t_, p_t_.T[:, 0], ls=\"--\", label=\"realization from posterior\")\n",
    "plt.plot(t_, p_t_.T[:, -8], ls=\"--\", label=\"realization from posterior\")\n",
    "plt.scatter(temperature_, D_, color=\"k\", s=50, alpha=0.5)\n",
    "plt.title(\"Posterior expected value of probability of defect; \\\n",
    "plus realizations\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.xlim(t_.min(), t_.max())\n",
    "plt.ylabel(\"probability\")\n",
    "plt.xlabel(\"temperature\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iI7Fosv1IA0T"
   },
   "source": [
    "Above we also plotted two possible realizations of what the actual underlying system might be. Both are equally likely as any other draw. The blue line is what occurs when we average all the 20000 possible dotted lines together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:58.017Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "XpRl_4Beof6G",
    "outputId": "58b09484-4510-4a8a-e16d-75d4393eace2"
   },
   "outputs": [],
   "source": [
    "alpha_samples_means_ = np.array(alpha_samples_1d_.mean(axis=1))\n",
    "beta_samples_means_ = np.array(beta_samples_1d_.mean(axis=1))\n",
    "sorted_alpha_means_ = np.sort(alpha_samples_means_)\n",
    "sorted_beta_means_ = np.sort(beta_samples_means_)\n",
    "alpha_index_ = sorted_alpha_means_.shape[0]\n",
    "beta_index_ = sorted_beta_means_.shape[0]\n",
    "upper_alpha_quantile_ix_ = int(alpha_index_ * float(0.975))\n",
    "lower_alpha_quantile_ix_ = int(alpha_index_ * float(0.025))\n",
    "\n",
    "upper_beta_quantile_ix_ = int(beta_index_ * float(0.975))\n",
    "lower_beta_quantile_ix_ = int(beta_index_ * float(0.025))\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "alpha_upper_quantile_ix_ = find_nearest(alpha_samples_means_, sorted_alpha_means_[upper_alpha_quantile_ix_])\n",
    "alpha_lower_quantile_ix_ = find_nearest(alpha_samples_means_, sorted_alpha_means_[lower_alpha_quantile_ix_])\n",
    "beta_upper_quantile_ix_ = find_nearest(beta_samples_means_, sorted_beta_means_[upper_beta_quantile_ix_])\n",
    "beta_lower_quantile_ix_ = find_nearest(beta_samples_means_, sorted_beta_means_[lower_beta_quantile_ix_])\n",
    "\n",
    "p_t_low = logistic(t_.T, beta_samples_1d_[beta_lower_quantile_ix_], alpha_samples_1d_[alpha_lower_quantile_ix_])\n",
    "p_t_high = logistic(t_.T, beta_samples_1d_[beta_upper_quantile_ix_], alpha_samples_1d_[alpha_upper_quantile_ix_])\n",
    "\n",
    "[ \n",
    "    p_t_low_, p_t_high_\n",
    "] = evaluate([ \n",
    "    p_t_low, p_t_high\n",
    "])\n",
    "qs = np.stack([p_t_low_[0][::50], p_t_high_[0][::50]], axis=0)\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1, sharex=True)\n",
    "\n",
    "ax1.fill_between(t_[::50].T[0], qs[0], qs[1], alpha=0.7, color=TFColor[6])\n",
    "plt.plot(t_[::50].T[0], qs[0], label=\"95% CI\", color=TFColor[6], alpha=0.7)\n",
    "\n",
    "plt.plot(t_.T[0][::50], mean_prob_t_[0][::50], lw=1, ls=\"--\", color=\"k\",\n",
    "         label=\"average posterior \\nprobability of defect\")\n",
    "\n",
    "\n",
    "\n",
    "plt.xlim(t_.min(), t_.max())\n",
    "plt.ylim(-0.02, 1.02)\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.scatter(temperature_, D_, color=\"k\", s=50, alpha=0.5)\n",
    "plt.xlabel(\"temp, $t$\")\n",
    "\n",
    "plt.ylabel(\"probability estimate\")\n",
    "plt.title(\"Posterior probability estimates given temp. $t$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rtcc9pBFIA0X"
   },
   "source": [
    "The 95% credible interval, or 95% CI, painted in purple, represents the interval, for each temperature, that contains 95% of the distribution. For example, at 65 degrees, we can be 95% sure that the probability of defect lies between 0.25 and 0.75.\n",
    "\n",
    "More generally, we can see that as the temperature nears 60 degrees, the CI's spread out over $[0,1]$ quickly. As we pass 70 degrees, the CI's tighten again. This can give us insight about how to proceed next: we should probably test more O-rings around 60-65 temperature to get a better estimate of probabilities in that range. Similarly, when reporting to scientists your estimates, you should be very cautious about simply telling them the expected probability, as we can see this does not reflect how *wide* the posterior distribution is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lesq_3oIIA0Y"
   },
   "source": [
    "### What about the day of the Challenger disaster?\n",
    "\n",
    "On the day of the Challenger disaster, the outside temperature was 31 degrees Fahrenheit. What is the posterior distribution of a defect occurring,  given this temperature? The distribution is plotted below. It looks almost guaranteed that the Challenger was going to be subject to defective O-rings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:58.028Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "AYbamYmdUdBZ",
    "outputId": "36119446-ebdd-47c8-95d3-3fb29e86c041"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize(12.5, 3))\n",
    "\n",
    "prob_31 = logistic(31, beta_samples_, alpha_samples_)\n",
    "\n",
    "[ prob_31_ ] = evaluate([ prob_31 ])\n",
    "#print(prob_31_)\n",
    "\n",
    "plt.xlim(0.995, 1)  # This should be changed to plt.xlim(0.995, 1), but illustrates the error\n",
    "plt.hist(prob_31_, bins=10, normed=True, histtype='stepfilled')\n",
    "plt.title(\"Posterior distribution of probability of defect, given $t = 31$\")\n",
    "plt.xlabel(\"probability of defect occurring in O-ring\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WjAFZ8W9IA0c"
   },
   "source": [
    "### Is our model appropriate?\n",
    "\n",
    "The skeptical reader will say \"You deliberately chose the logistic function for $p(t)$ and the specific priors. Perhaps other functions or priors will give different results. How do I know I have chosen a good model?\" This is absolutely true. To consider an extreme situation, what if I had chosen the function $p(t) = 1,\\; \\forall t$, which guarantees a defect always occurring: I would have again predicted disaster on January 28th. Yet this is clearly a poorly chosen model. On the other hand, if I did choose the logistic function for $p(t)$, but specified all my priors to be very tight around 0, likely we would have very different posterior distributions. How do we know our model is an expression of the data? This encourages us to measure the model's **goodness of fit**.\n",
    "\n",
    "We can think: *how can we test whether our model is a bad fit?* An idea is to compare observed data with artificial dataset which we can simulate. The rationale is that if the simulated dataset does not appear similar, statistically, to the observed dataset, then likely our model is not accurately represented the observed data. \n",
    "\n",
    "Previously in this Chapter, we simulated an artificial dataset for the SMS example. To do this, we sampled values from the priors. We saw how varied the resulting datasets looked like, and rarely did they mimic our observed dataset. In the current example,  we should sample from the *posterior* distributions to create *very plausible datasets*. Luckily, our Bayesian framework makes this very easy. We only need to gather samples from the distribution of choice, and specify the number of samples, the shape of the samples (we had 21 observations in our original dataset, so we'll make the shape of each sample 21), and the probability we want to use to determine the ratio of 1 observations to 0 observations.\n",
    "\n",
    "\n",
    "Hence we create the following:\n",
    "\n",
    "```python\n",
    "simulated_data = tfd.Bernoulli(name=\"simulation_data\", probs=p).sample(sample_shape=N)\n",
    "```\n",
    "Let's simulate 10 000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:58.039Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MvFwyz9hwROg"
   },
   "outputs": [],
   "source": [
    "alpha = alpha_mean_ # We're basing these values on the outputs of our model above\n",
    "beta = beta_mean_\n",
    "p_deterministic = tfd.Deterministic(name=\"p\", loc=1.0/(1. + tf.exp(beta * temperature_ + alpha))).sample()#seed=6.45)\n",
    "simulated_data = tfd.Bernoulli(name=\"bernoulli_sim\", \n",
    "                               probs=p_deterministic_).sample(sample_shape=10000)\n",
    "[ \n",
    "    bernoulli_sim_samples_,\n",
    "    p_deterministic_\n",
    "] =evaluate([\n",
    "    simulated_data,\n",
    "    p_deterministic\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:58.048Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "id": "gDyVY1wmgjx4",
    "outputId": "d48ffb1b-57bf-41e5-eda5-4289bef1ad4a"
   },
   "outputs": [],
   "source": [
    "simulations_ = bernoulli_sim_samples_\n",
    "print(\"Number of simulations:             \", simulations_.shape[0])\n",
    "print(\"Number data points per simulation: \", simulations_.shape[1])\n",
    "\n",
    "plt.figure(figsize(12.5, 12))\n",
    "plt.title(\"Simulated dataset using posterior parameters\")\n",
    "for i in range(4):\n",
    "    ax = plt.subplot(4, 1, i+1)\n",
    "    plt.scatter(temperature_, simulations_[1000*i, :], color=\"k\",\n",
    "                s=50, alpha=0.6)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RFSaMzNQIA0l"
   },
   "source": [
    "Note that the above plots are different (if you can think of a cleaner way to present this, please send a pull request and answer [here](http://stats.stackexchange.com/questions/53078/how-to-visualize-bayesian-goodness-of-fit-for-logistic-regression)!).\n",
    "\n",
    "We wish to assess how good our model is. \"Good\" is a subjective term of course, so results must be relative to other models. \n",
    "\n",
    "We will be doing this graphically as well, which may seem like an even less objective method. The alternative is to use *Bayesian p-values*. These are still subjective, as the proper cutoff between good and bad is arbitrary. Gelman emphasises that the graphical tests are more illuminating [3] than p-value tests. We agree.\n",
    "\n",
    "The following graphical test is a novel data-viz approach to logistic regression. The plots are called *separation plots*[4]. For a suite of models we wish to compare, each model is plotted on an individual separation plot. I leave most of the technical details about separation plots to the very accessible [original paper](http://mdwardlab.com/sites/default/files/GreenhillWardSacks.pdf), but I'll summarize their use here.\n",
    "\n",
    "For each model, we calculate the proportion of times the posterior simulation proposed a value of 1 for a particular temperature, i.e. compute $P( \\;\\text{Defect} = 1 | t, \\alpha, \\beta )$ by averaging. This gives us the posterior probability of a defect at each data point in our dataset. For example, for the model we used above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:58.058Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "3ho1cPLAIA0l",
    "outputId": "753be416-5530-43c0-e44c-bb5d62dfc358"
   },
   "outputs": [],
   "source": [
    "posterior_probability_ = simulations_.mean(axis=0)\n",
    "print(\"posterior prob of defect | realized defect \")\n",
    "for i in range(len(D_)):\n",
    "    print(\"%.2f                     |   %d\" % (posterior_probability_[i], D_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-q4yysOiIA0n"
   },
   "source": [
    "Next we sort each column by the posterior probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:58.068Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "i3TkXUhSIA0n",
    "outputId": "3de583e0-0204-4fa1-f67b-caee128c5569"
   },
   "outputs": [],
   "source": [
    "ix_ = np.argsort(posterior_probability_)\n",
    "print(\"probb | defect \")\n",
    "for i in range(len(D_)):\n",
    "    print(\"%.2f  |   %d\" % (posterior_probability_[ix_[i]], D_[ix_[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ajvopQADIA0p"
   },
   "source": [
    "We can present the above data better in a figure: we've creates a `separation_plot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:58.078Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "tFR1_yu8IA0p",
    "outputId": "e5508fb9-0448-483c-d42e-157d82b60500"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def separation_plot( p, y, **kwargs ):\n",
    "    \"\"\"\n",
    "    This function creates a separation plot for logistic and probit classification. \n",
    "    See http://mdwardlab.com/sites/default/files/GreenhillWardSacks.pdf\n",
    "    \n",
    "    p: The proportions/probabilities, can be a nxM matrix which represents M models.\n",
    "    y: the 0-1 response variables.\n",
    "    \n",
    "    \"\"\"    \n",
    "    assert p.shape[0] == y.shape[0], \"p.shape[0] != y.shape[0]\"\n",
    "    n = p.shape[0]\n",
    "\n",
    "    try:\n",
    "        M = p.shape[1]\n",
    "    except:\n",
    "        p = p.reshape( n, 1 )\n",
    "        M = p.shape[1]\n",
    "\n",
    "    colors_bmh = np.array( [\"#eeeeee\", \"#348ABD\"] )\n",
    "\n",
    "\n",
    "    fig = plt.figure( )\n",
    "    \n",
    "    for i in range(M):\n",
    "        ax = fig.add_subplot(M, 1, i+1)\n",
    "        ix = np.argsort( p[:,i] )\n",
    "        #plot the different bars\n",
    "        bars = ax.bar( np.arange(n), np.ones(n), width=1.,\n",
    "                color = colors_bmh[ y[ix].astype(int) ], \n",
    "                edgecolor = 'none')\n",
    "        ax.plot( np.arange(n+1), np.append(p[ix,i], p[ix,i][-1]), \"k\",\n",
    "                 linewidth = 1.,drawstyle=\"steps-post\" )\n",
    "        #create expected value bar.\n",
    "        ax.vlines( [(1-p[ix,i]).sum()], [0], [1] )\n",
    "        plt.xlim( 0, n)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return\n",
    "\n",
    "plt.figure(figsize(11., 3))\n",
    "separation_plot(posterior_probability_, D_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBuY2lSaIA0s"
   },
   "source": [
    "The snaking-line is the sorted probabilities, blue bars denote defects, and empty space (or grey bars for the optimistic readers) denote non-defects.  As the probability rises, we see more and more defects occur. On the right hand side, the plot suggests that as the posterior probability is large (line close to 1), then more defects are realized. This is good behaviour. Ideally, all the blue bars *should* be close to the right-hand side, and deviations from this reflect missed predictions. \n",
    "\n",
    "The black vertical line is the expected number of defects we should observe, given this model. This allows the user to see how the total number of events predicted by the model compares to the actual number of events in the data.\n",
    "\n",
    "It is much more informative to compare this to separation plots for other models. Below we compare our model (top) versus three others:\n",
    "\n",
    "1. the perfect model, which predicts the posterior probability to be equal 1 if a defect did occur.\n",
    "2. a completely random model, which predicts random probabilities regardless of temperature.\n",
    "3. a constant model:  where $P(D = 1 \\; | \\; t) = c, \\;\\; \\forall t$. The best choice for $c$ is the observed frequency of defects, in this case 7/23.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:58.088Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "colab_type": "code",
    "id": "RbX1nHrBIA0s",
    "outputId": "d705df41-b68e-4f98-983a-df76934e9ffc"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize(11., 2))\n",
    "\n",
    "# Our temperature-dependent model\n",
    "separation_plot(posterior_probability_, D_)\n",
    "plt.title(\"Temperature-dependent model\")\n",
    "\n",
    "# Perfect model\n",
    "# i.e. the probability of defect is equal to if a defect occurred or not.\n",
    "p_ = D_\n",
    "separation_plot(p_, D_)\n",
    "plt.title(\"Perfect model\")\n",
    "\n",
    "# random predictions\n",
    "p_ = np.random.rand(23)\n",
    "separation_plot(p_, D_)\n",
    "plt.title(\"Random model\")\n",
    "\n",
    "# constant model\n",
    "constant_prob_ = 7./23 * np.ones(23)\n",
    "separation_plot(constant_prob_, D_)\n",
    "plt.title(\"Constant-prediction model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etHuMg8OIA0u"
   },
   "source": [
    "In the random model, we can see that as the probability increases there is no clustering of defects to the right-hand side. Similarly for the constant model.\n",
    "\n",
    "In the perfect model, the probability line is not well shown, as it is stuck to the bottom and top of the figure. Of course the perfect model is only for demonstration, and we cannot infer any scientific inference from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_I_3h7lsIA0v"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1\\. Try putting in extreme values for our observations in the cheating example. What happens if we observe 25 affirmative responses? 10? 50? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:58.100Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "_B-K0Neyx7pZ"
   },
   "outputs": [],
   "source": [
    "#type your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ulECHYwMIA0v"
   },
   "source": [
    "2\\. Try plotting $\\alpha$ samples versus $\\beta$ samples.  Why might the resulting plot look like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:58.113Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "21v-EuHZIA0v",
    "outputId": "ac7f87d4-4b14-45c4-ace4-dd2e7aa2eb28"
   },
   "outputs": [],
   "source": [
    "#type your code here.\n",
    "plt.figure(figsize(12.5, 4))\n",
    "\n",
    "plt.scatter(alpha_samples_, beta_samples_, alpha=0.1)\n",
    "plt.title(\"Why does the plot look like this?\")\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(r\"$\\beta$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e6RYS1_jIA0y"
   },
   "source": [
    "## References\n",
    "\n",
    "[1] Dalal, Fowlkes and Hoadley (1989),JASA, 84, 945-957.\n",
    "\n",
    "[2] Cronin, Beau. \"Why Probabilistic Programming Matters.\" 24 Mar 2013. Google, Online Posting to Google . Web. 24 Mar. 2013. <https://plus.google.com/u/0/+BeauCronin/posts/KpeRdJKR6Z1>.\n",
    "\n",
    "[3] Gelman, Andrew. \"Philosophy and the practice of Bayesian statistics.\" British Journal of Mathematical and Statistical Psychology. (2012): n. page. Web. 2 Apr. 2013.\n",
    "\n",
    "[4] Greenhill, Brian, Michael D. Ward, and Audrey Sacks. \"The Separation Plot: A New Visual Method for Evaluating the Fit of Binary Models.\" American Journal of Political Science. 55.No.4 (2011): n. page. Web. 2 Apr. 2013.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T15:42:58.125Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "TxfPXLi0IA0z"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Ch2_MorePyMC_TFP.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
